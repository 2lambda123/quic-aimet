{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Layer Equalization and Bias Correction Example Code\n",
    "\n",
    "This script utilizes AIMET to apply Cross Layer Equalization and Bias Correction on a resnet18.The general procedure for quantization is to optionally change the model through Cross-Layer Equalization and/or Bias Correction, then use AIMET's QuantizationSimModel to compute new encodings, then finetune the model. Here is an overview of each featue this notebook showcases.\n",
    "\n",
    "**Cross Layer Equalization**  \n",
    "1. Batch Norm Folding: accounting for the parameters of batch norm layers by changing the weights of the subsequent convolutional layers\n",
    "2. Cross-Layer Scaling: rescaling the weights of consecutive convolutional layers to make them closer in magnitude to one another\n",
    "3. High Bias Folding: redistributing the high biases of some layers to other layers with lower biases.\n",
    "\n",
    "**Bias Correction**  \n",
    "Bias Correction is used to make sure the mean of the outputs of a given layer is the same before and after the quantization step. This is done simply by taking the quantized bias and subtracting the expected difference between the outputs associated with the original weights and the quantized weights.\n",
    "\n",
    "\n",
    "#### The example code shows the following:\n",
    "1. Instantiate Data Pipeline for evaluation \n",
    "2. Load the pretrained resnet18 Pytorch model\n",
    "3. Calculate Model accuracy\n",
    "    * 3.1. Calculate floating point accuracy\n",
    "    * 3.2. Calculate Quant Simulator accuracy\n",
    "4. Apply AIMET CLE and BC\n",
    "    * 4.1. Apply AIMET CLE and calculates QuantSim accuracy\n",
    "    * 4.2. Apply AIMET BC and calculates QuantSim accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \".*param.*\")\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import argparse\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import torch\n",
    "from torchvision.models import resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-19 15:38:15,510 - root - INFO - aimetpro-1.20.0_Build_Id_0.139.0.1838.torch-gpu-universal\n"
     ]
    }
   ],
   "source": [
    "# AIMET Imports for Quantization\n",
    "from aimet_common.defs import QuantScheme\n",
    "from aimet_torch.quantsim import QuantizationSimModel, QuantParams\n",
    "from aimet_torch.bias_correction import correct_bias\n",
    "from aimet_torch.cross_layer_equalization import equalize_model\n",
    "from aimet_torch.batch_norm_fold import fold_all_batch_norms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Pipeline Imports\n",
    "from Examples.common import image_net_config\n",
    "from Examples.torch.utils.image_net_evaluator import ImageNetEvaluator\n",
    "from Examples.torch.utils.image_net_trainer import ImageNetTrainer\n",
    "from Examples.torch.utils.image_net_evaluator import ImageNetDataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Our Config Dictionary\n",
    "\n",
    "The config dictionary specifies a number of things \n",
    "\n",
    "config: \n",
    "This mapping expects following parameters:\n",
    "1. **dataset_dir:** Path to a directory containing ImageNet dataset. This folder should contain subfolders 'train' for training dataset and 'val' for validation dataset.\n",
    "3. **use_cuda:** A boolean var to indicate to run the quantization on GPU.\n",
    "4. **logdir:** Path to a directory for logging.\n",
    "\n",
    "To get a better understanding of when each of the parameters in the config dictionary is used, read the code in those cells.  \n",
    "**Note:** You will have to replace the dataset_dir path with the path to your own imagenet/tinyimagenet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'dataset_dir': \"path/to/dataset\",\n",
    "          'use_cuda': True,\n",
    "          'logdir': os.path.join(\"benchmark_output\", \"cle_bc_\"+datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))}\n",
    "\n",
    "os.makedirs(config['logdir'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instantiate Data Pipeline\n",
    "\n",
    "The ImageNetDataPipeline class takes care of evaluating a model using a dataset directory. For more detail on how it works, see the relevant files under examples/torch/utils.\n",
    "\n",
    "The data pipeline class is simply a template for the user to follow. The methods for this class can be replaced by the user to fit their needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetDataPipeline:\n",
    "    \"\"\"\n",
    "    Provides APIs for model quantization using evaluation and finetuning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        :param config:\n",
    "        \"\"\"\n",
    "        self._config = config\n",
    "\n",
    "    def data_loader(self):\n",
    "        \"\"\"\n",
    "        :return: ImageNetDataloader\n",
    "        \"\"\"\n",
    "        \n",
    "        data_loader = ImageNetDataLoader(is_training=False, images_dir=self._config[\"dataset_dir\"],\n",
    "                                         image_size=image_net_config.dataset['image_size']).data_loader\n",
    "\n",
    "        return data_loader\n",
    "    \n",
    "    def evaluate(self, model: torch.nn.Module, iterations: int = None, use_cuda: bool = False) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate the specified model using the specified number of samples from the validation set.\n",
    "        :param model: The model to be evaluated.\n",
    "        :param iterations: The number of batches of the dataset.\n",
    "        :param use_cuda: If True then use a GPU for inference.\n",
    "        :return: The accuracy for the sample with the maximum accuracy.\n",
    "        \"\"\"\n",
    "\n",
    "        # Your code goes here\n",
    "\n",
    "        evaluator = ImageNetEvaluator(self._config['dataset_dir'], image_size=image_net_config.dataset['image_size'],\n",
    "                                      batch_size=image_net_config.evaluation['batch_size'],\n",
    "                                      num_workers=image_net_config.evaluation['num_workers'])\n",
    "\n",
    "        return evaluator.evaluate(model, iterations, use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Model, Initialize DataPipeline\n",
    "\n",
    "The next section will initialize the model and data pipeline for the quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the pipeline and the model. Before quantizing the model, we calculate the original floating point (FP32) accuracy of the model on the dataset provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-19 15:38:22,046 - Dataloader - INFO - Dataset consists of 1000 images in 1000 classes\n",
      "2022-01-19 15:38:22,051 - Eval - INFO - No value of iteration is provided, running evaluation on complete dataset.\n",
      "2022-01-19 15:38:22,052 - Eval - INFO - Evaluating nn.Module for 4 iterations with batch_size 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "100% (4 of 4) |##########################| Elapsed Time: 0:00:01 Time:  0:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-19 15:38:26,540 - Eval - INFO - Avg accuracy Top 1: 68.655710 Avg accuracy Top 5: 88.574219 on validation Dataset\n",
      "Original Model Accuracy:  68.65571022033691\n"
     ]
    }
   ],
   "source": [
    "data_pipeline = ImageNetDataPipeline(config)\n",
    "\n",
    "model = resnet18(pretrained=True)\n",
    "if config['use_cuda']:\n",
    "    if torch.cuda.is_available():\n",
    "        model.to(torch.device('cuda'))\n",
    "    else:\n",
    "        raise Exception(\"use_cuda is True but cuda is unavailable\")\n",
    "model.eval()\n",
    "\n",
    "accuracy = data_pipeline.evaluate(model, use_cuda=config['use_cuda'])\n",
    "print(\"Original Model Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quantization Simulator\n",
    "\n",
    "The next cells are for the actual quantization step. The quantization parameters are specified in the following cell:\n",
    "\n",
    "1. **quant_scheme**: The scheme used to quantize the model. We can choose from s - post_training_tf or post_training_tf_enhanced.\n",
    "\n",
    "2. **rounding_mode**: The rounding mode used for quantization. There are two possible choices here - 'nearest' or 'stochastic'\n",
    "\n",
    "3. **default_output_bw**: The bitwidth of the activation tensors. The value of this should be a power of 2, less than 32.\n",
    "\n",
    "4. **default_param_bw**: The bidwidth of the parameter tensors. The value of this should be a power of 2, less than 32.\n",
    "\n",
    "5. **num_batches**: The number of batches used to evaluate the model while calculating the quantization encodings.Number of batches to use for computing encodings. Only 5 batches are used here to speed up the process. In addition, the number of images in these 5 batches should be sufficient for compute encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_scheme = QuantScheme.post_training_tf_enhanced\n",
    "\n",
    "rounding_mode = 'nearest'\n",
    "\n",
    "default_output_bw = 8\n",
    "\n",
    "default_param_bw = 8\n",
    "\n",
    "#Uncomment one of the following lines\n",
    "# num_batches = 5 #Typical\n",
    "num_batches = 1 #Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set up the quantization simulator, and quantize the model. The resulting quantized (INT8) Model is then evaluated on the dataset. We utilize the evaluate function from the data pipeline to compute the new weights.\n",
    "\n",
    "it is customary to fold batch norms; however, the Cross Layer Equalization API expects a model which does not have folded batch norms. For this reason, we make a copy of our model to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-19 15:38:26,875 - Utils - INFO - ...... subset to store [Conv_0, BatchNormalization_1]\n",
      "2022-01-19 15:38:26,876 - Utils - INFO - ...... subset to store [Conv_4, BatchNormalization_5]\n",
      "2022-01-19 15:38:26,876 - Utils - INFO - ...... subset to store [Conv_7, BatchNormalization_8]\n",
      "2022-01-19 15:38:26,877 - Utils - INFO - ...... subset to store [Conv_11, BatchNormalization_12]\n",
      "2022-01-19 15:38:26,877 - Utils - INFO - ...... subset to store [Conv_14, BatchNormalization_15]\n",
      "2022-01-19 15:38:26,878 - Utils - INFO - ...... subset to store [Conv_18, BatchNormalization_19]\n",
      "2022-01-19 15:38:26,878 - Utils - INFO - ...... subset to store [Conv_21, BatchNormalization_22]\n",
      "2022-01-19 15:38:26,878 - Utils - INFO - ...... subset to store [Conv_27, BatchNormalization_28]\n",
      "2022-01-19 15:38:26,879 - Utils - INFO - ...... subset to store [Conv_30, BatchNormalization_31]\n",
      "2022-01-19 15:38:26,879 - Utils - INFO - ...... subset to store [Conv_34, BatchNormalization_35]\n",
      "2022-01-19 15:38:26,879 - Utils - INFO - ...... subset to store [Conv_37, BatchNormalization_38]\n",
      "2022-01-19 15:38:26,880 - Utils - INFO - ...... subset to store [Conv_43, BatchNormalization_44]\n",
      "2022-01-19 15:38:26,880 - Utils - INFO - ...... subset to store [Conv_46, BatchNormalization_47]\n",
      "2022-01-19 15:38:26,881 - Utils - INFO - ...... subset to store [Conv_50, BatchNormalization_51]\n",
      "2022-01-19 15:38:26,881 - Utils - INFO - ...... subset to store [Conv_53, BatchNormalization_54]\n",
      "2022-01-19 15:38:26,881 - Utils - INFO - ...... subset to store [Conv_59, BatchNormalization_60]\n",
      "2022-01-19 15:38:26,882 - Utils - INFO - ...... subset to store [Conv_62, BatchNormalization_63]\n",
      "2022-01-19 15:38:26,882 - Utils - INFO - ...... subset to store [Conv_55, BatchNormalization_56]\n",
      "2022-01-19 15:38:26,883 - Utils - INFO - ...... subset to store [Conv_39, BatchNormalization_40]\n",
      "2022-01-19 15:38:26,883 - Utils - INFO - ...... subset to store [Conv_23, BatchNormalization_24]\n",
      "2022-01-19 15:38:29,864 - Quant - INFO - No config file provided, defaulting to config file at /usr/local/lib/python3.6/dist-packages/aimet_common/quantsim_config/default_config.json\n",
      "2022-01-19 15:38:29,871 - Utils - INFO - ...... subset to store [Conv_0, Relu_1]\n",
      "2022-01-19 15:38:29,872 - Utils - INFO - ...... subset to store [Conv_3, Relu_4]\n",
      "2022-01-19 15:38:29,872 - Utils - INFO - ...... subset to store [Add_6, Relu_7]\n",
      "2022-01-19 15:38:29,873 - Utils - INFO - ...... subset to store [Conv_8, Relu_9]\n",
      "2022-01-19 15:38:29,873 - Utils - INFO - ...... subset to store [Add_11, Relu_12]\n",
      "2022-01-19 15:38:29,874 - Utils - INFO - ...... subset to store [Conv_13, Relu_14]\n",
      "2022-01-19 15:38:29,875 - Utils - INFO - ...... subset to store [Add_17, Relu_18]\n",
      "2022-01-19 15:38:29,875 - Utils - INFO - ...... subset to store [Conv_19, Relu_20]\n",
      "2022-01-19 15:38:29,876 - Utils - INFO - ...... subset to store [Add_22, Relu_23]\n",
      "2022-01-19 15:38:29,877 - Utils - INFO - ...... subset to store [Conv_24, Relu_25]\n",
      "2022-01-19 15:38:29,877 - Utils - INFO - ...... subset to store [Add_28, Relu_29]\n",
      "2022-01-19 15:38:29,878 - Utils - INFO - ...... subset to store [Conv_30, Relu_31]\n",
      "2022-01-19 15:38:29,878 - Utils - INFO - ...... subset to store [Add_33, Relu_34]\n",
      "2022-01-19 15:38:29,879 - Utils - INFO - ...... subset to store [Conv_35, Relu_36]\n",
      "2022-01-19 15:38:29,880 - Utils - INFO - ...... subset to store [Add_39, Relu_40]\n",
      "2022-01-19 15:38:29,880 - Utils - INFO - ...... subset to store [Conv_41, Relu_42]\n",
      "2022-01-19 15:38:29,881 - Utils - INFO - ...... subset to store [Add_44, Relu_45]\n",
      "2022-01-19 15:38:30,758 - Dataloader - INFO - Dataset consists of 1000 images in 1000 classes\n",
      "2022-01-19 15:38:30,760 - Eval - INFO - Evaluating nn.Module for 1 iterations with batch_size 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (1 of 1) |##########################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-19 15:38:51,812 - Eval - INFO - Avg accuracy Top 1: 75.390625 Avg accuracy Top 5: 94.531250 on validation Dataset\n",
      "2022-01-19 15:38:52,719 - Dataloader - INFO - Dataset consists of 1000 images in 1000 classes\n",
      "2022-01-19 15:38:52,721 - Eval - INFO - No value of iteration is provided, running evaluation on complete dataset.\n",
      "2022-01-19 15:38:52,722 - Eval - INFO - Evaluating nn.Module for 4 iterations with batch_size 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50% (2 of 4) |#############             | Elapsed Time: 0:00:21 ETA:   0:00:21"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.rand(1, 3, 224, 224)\n",
    "if config['use_cuda']:\n",
    "    dummy_input = dummy_input.to(torch.device('cuda'))\n",
    "\n",
    "\n",
    "BN_folded_model = copy.deepcopy(model)\n",
    "_ = fold_all_batch_norms(BN_folded_model, input_shapes=(1, 3, 224, 224))\n",
    "\n",
    "quantizer = QuantizationSimModel(model=BN_folded_model,\n",
    "                                 quant_scheme=quant_scheme,\n",
    "                                 dummy_input=dummy_input,\n",
    "                                 rounding_mode=rounding_mode,\n",
    "                                 default_output_bw=default_output_bw,\n",
    "                                 default_param_bw=default_param_bw)\n",
    "\n",
    "quantizer.compute_encodings(forward_pass_callback=partial(data_pipeline.evaluate,\n",
    "                                                          use_cuda=config['use_cuda']),\n",
    "                            forward_pass_callback_args=num_batches)\n",
    "\n",
    "# Calculate quantized (INT8) accuracy after CLE\n",
    "accuracy = data_pipeline.evaluate(quantizer.model)\n",
    "print(\"Quantized (INT8) Model Top-1 Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 1 Cross Layer Equalization\n",
    "\n",
    "The next cell performs cross-layer equalization on the model. As noted before, the function folds batch norms, applies cross-layer scaling, and then folds high biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This API will equalize the model in-place\n",
    "equalize_model(model, input_shapes=(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the model is quantized, and the accuracy is noted. This is done before the bias correction step in order to measure the individual impacts of each technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.rand(1, 3, 224, 224)\n",
    "if config['use_cuda']:\n",
    "    dummy_input = dummy_input.to(torch.device('cuda'))\n",
    "\n",
    "cle_quantizer = QuantizationSimModel(model=model,\n",
    "                                     quant_scheme=quant_scheme,\n",
    "                                     dummy_input=dummy_input,\n",
    "                                     rounding_mode=rounding_mode,\n",
    "                                     default_output_bw=default_output_bw,\n",
    "                                     default_param_bw=default_param_bw)\n",
    "\n",
    "cle_quantizer.compute_encodings(forward_pass_callback=partial(data_pipeline.evaluate,\n",
    "                                                              use_cuda=config['use_cuda']),\n",
    "                                forward_pass_callback_args=num_batches)\n",
    "\n",
    "accuracy = data_pipeline.evaluate(cle_quantizer.model)\n",
    "print(\"CLE applied Model Top-1 accuracy on Quant Simulator: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 2 Bias Correction\n",
    "\n",
    "Perform Bias correction and calculate the accuracy on the quantsim model. The first cell includes two parameters related to this step:\n",
    "\n",
    "1. **num_quant_samples**: The number of samples used during quantization\n",
    "2. **num_bias_correction_samples**: The number of samples used during bias correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment one of the following sets of parameters\n",
    "num_quant_samples = 16 #Typical\n",
    "num_bias_correct_samples = 16 #Typical\n",
    "\n",
    "num_quant_samples = 1 #Test\n",
    "num_bias_correct_samples = 1 #Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the actual bias correction steps are performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = data_pipeline.data_loader()\n",
    "\n",
    "bc_params = QuantParams(weight_bw=default_param_bw,\n",
    "                        act_bw=default_output_bw,\n",
    "                        round_mode=rounding_mode,\n",
    "                        quant_scheme=quant_scheme)\n",
    "\n",
    "correct_bias(model,\n",
    "             bc_params,\n",
    "             num_quant_samples=num_quant_samples,\n",
    "             data_loader=data_loader,\n",
    "             num_bias_correct_samples=num_bias_correct_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the model is quantized, the accuracy is logged, and the model is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.rand(1, 3, 224, 224)\n",
    "if config['use_cuda']:\n",
    "    dummy_input = dummy_input.to(torch.device('cuda'))\n",
    "\n",
    "bc_quantizer = QuantizationSimModel(model=model,\n",
    "                                    quant_scheme=quant_scheme,\n",
    "                                    dummy_input=dummy_input,\n",
    "                                    rounding_mode=rounding_mode,\n",
    "                                    default_output_bw=default_output_bw,\n",
    "                                    default_param_bw=default_param_bw,\n",
    "                                    in_place=False)\n",
    "\n",
    "bc_quantizer.compute_encodings(forward_pass_callback=partial(data_pipeline.evaluate,\n",
    "                                                             use_cuda=config['use_cuda']),\n",
    "                               forward_pass_callback_args=num_batches)\n",
    "\n",
    "accuracy = data_pipeline.evaluate(bc_quantizer.model)\n",
    "print(\"Quantized (INT8) Model Top-1 Accuracy After Bias Correction: \", accuracy)\n",
    "\n",
    "torch.save(model, os.path.join(config['logdir'], 'quantized_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "For more information on how Cross Layer Equalization and Bias Correction works, be sure to check out this [page](https://quic.github.io/aimet-pages/AimetDocs/user_guide/post_training_quant_techniques.html#ug-post-training-quantization) on post-training quantization techniques and this [paper](https://arxiv.org/abs/1906.04721) on Cross Layer Equalization and Bias Correction.\n",
    "\n",
    "For more information about AIMET's APIs, visit the [documentation](https://quic.github.io/aimet-pages/AimetDocs/api_docs/torch_quantization.html) on Torch Model Quantization. For a better understanding on what AIMET has to offer, be sure to check out this [YouTube playlist](https://www.youtube.com/playlist?list=PLd0XF75dq-1a7OZTl1kAiM2ZqeKqQpKFH), and this [page](https://quic.github.io/aimet-pages/index.html) on AIMET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
