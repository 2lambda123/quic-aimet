{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization Aware Training Example Code\n",
    "\n",
    "The following notebook is an example to show quantization simulation and finetuning using the AIMET library. The general procedure for quantization is to use AIMET's QuantizationSimModel to compute new encodings, then finetune the model.\n",
    "\n",
    "We now present an overview of the technique. The weights of the pretrained model (in our case, ResNet), are originally 32-bit floating point numbers, which are then converted to 8-bit numbers through a rounding procedure. Then, this quantized model can be finetuned.\n",
    "\n",
    "\n",
    "This script utilizes AIMET to perform Quantization Aware Training on a ResNet18 pretrained model with the ImageNet data set. This is intended as a working example to show how AIMET APIs can be invoked.\n",
    "\n",
    "Scenario parameters:\n",
    "1. AIMET quantization aware training using simulation model\n",
    "2. Quant Scheme: 'tf'\n",
    "3. rounding_mode: 'nearest'\n",
    "4. default_output_bw: 8, default_param_bw: 8\n",
    "5. Encoding computation using 5 batches of data\n",
    "6. Input shape: [1, 3, 224, 224]\n",
    "7. Learning rate: 0.001\n",
    "8. Decay Steps: 5\n",
    "\n",
    "#### The example code shows the following:\n",
    "1. Instantiate Data Pipeline for evaluation \n",
    "2. Load the pretrained ResNet18 Pytorch model\n",
    "3. Calculate Model accuracy\n",
    "    * 3.1. Calculate floating point accuracy\n",
    "    * 3.2. Calculate Quant Simulator accuracy\n",
    "4. Apply AIMET CLE and BC\n",
    "    * 4.1. Apply AIMET CLE and calculates QuantSim accuracy\n",
    "    * 4.2. Apply AIMET BC and calculates QuantSim accuracy\n",
    "5. Fine-tune Model\n",
    "\n",
    "The first three cells below takes care of all necessary imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \".*param.*\")\n",
    "\n",
    "# Imports necessary for the notebook\n",
    "import os\n",
    "import copy\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import torch\n",
    "from torchvision.models import resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIMET Imports for Quantization\n",
    "from aimet_common.defs import QuantScheme\n",
    "from aimet_torch.quantsim import QuantizationSimModel, QuantParams\n",
    "from aimet_torch.bias_correction import correct_bias\n",
    "\n",
    "from aimet_torch.cross_layer_equalization import equalize_model\n",
    "from aimet_torch.batch_norm_fold import fold_all_batch_norms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports needed for the Data Pipeline\n",
    "from Examples.common import image_net_config\n",
    "from Examples.torch.utils.image_net_evaluator import ImageNetEvaluator\n",
    "from Examples.torch.utils.image_net_trainer import ImageNetTrainer\n",
    "from Examples.torch.utils.image_net_evaluator import ImageNetDataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Our Config Dictionary\n",
    "\n",
    "The config dictionary specifies a number of things \n",
    "\n",
    "config: \n",
    "This mapping expects following parameters:\n",
    "1. **dataset_dir:** Path to a directory containing ImageNet dataset. This folder should contain subfolders 'train' for training dataset and 'val' for validation dataset.\n",
    "3. **use_cuda:** A boolean var to indicate to run the quantization on GPU.\n",
    "4. **logdir:** Path to a directory for logging.\n",
    "\n",
    "To get a better understanding of when each of the parameters in the config dictionary is used, read the code in those cells.  \n",
    "**Note:** You will have to replace the dataset_dir path with the path to your own imagenet/tinyimagenet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'dataset_dir': \"path/to/dataset\",\n",
    "          'use_cuda': True,\n",
    "          'logdir': os.path.join(\"benchmark_output\", \"QAT_\"+datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")), \n",
    "          'epochs': 1, \n",
    "          'learning_rate': 1e-2, \n",
    "          'learning_rate_schedule': [5, 10]}\n",
    "\n",
    "os.makedirs(config['logdir'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instantiate Data Pipeline\n",
    "\n",
    "The ImageNetDataPipeline class takes care of evaluating a model using a dataset directory. For more detail on how it works, see the relevant files under examples/torch/utils.\n",
    "\n",
    "The data pipeline class is simply a template for the user to follow. The methods for this class can be replaced by the user to fit their needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetDataPipeline:\n",
    "    \"\"\"\n",
    "    Provides APIs for model quantization using evaluation and finetuning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        :param config:\n",
    "        \"\"\"\n",
    "        self._config = config\n",
    "\n",
    "    def data_loader(self):\n",
    "        \"\"\"\n",
    "        :return: ImageNetDataloader\n",
    "        \"\"\"\n",
    "        \n",
    "        data_loader = ImageNetDataLoader(is_training=False, images_dir=self._config[\"dataset_dir\"],\n",
    "                                         image_size=image_net_config.dataset['image_size']).data_loader\n",
    "\n",
    "        return data_loader\n",
    "    \n",
    "    def evaluate(self, model: torch.nn.Module, iterations: int = None, use_cuda: bool = False) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate the specified model using the specified number of samples from the validation set.\n",
    "        :param model: The model to be evaluated.\n",
    "        :param iterations: The number of batches of the dataset.\n",
    "        :param use_cuda: If True then use a GPU for inference.\n",
    "        :return: The accuracy for the sample with the maximum accuracy.\n",
    "        \"\"\"\n",
    "\n",
    "        # Your code goes here\n",
    "\n",
    "        evaluator = ImageNetEvaluator(self._config['dataset_dir'], image_size=image_net_config.dataset['image_size'],\n",
    "                                      batch_size=image_net_config.evaluation['batch_size'],\n",
    "                                      num_workers=image_net_config.evaluation['num_workers'])\n",
    "\n",
    "        return evaluator.evaluate(model, iterations, use_cuda)\n",
    "    \n",
    "    def finetune(self, model: torch.nn.Module):\n",
    "        \"\"\"\n",
    "        Finetunes the model.  The implemtation provided here is just an example,\n",
    "        provide your own implementation if needed.\n",
    "\n",
    "        :param model: The model to finetune.\n",
    "        \"\"\"\n",
    "\n",
    "        # Your code goes here instead of the example from below\n",
    "\n",
    "        trainer = ImageNetTrainer(self._config['dataset_dir'], image_size=image_net_config.dataset['image_size'],\n",
    "                                  batch_size=image_net_config.train['batch_size'],\n",
    "                                  num_workers=image_net_config.train['num_workers'])\n",
    "\n",
    "        trainer.train(model, max_epochs=self._config['epochs'], learning_rate=self._config['learning_rate'],\n",
    "                      learning_rate_schedule=self._config['learning_rate_schedule'], use_cuda=self._config['use_cuda'])\n",
    "\n",
    "        torch.save(model, os.path.join(self._config['logdir'], 'finetuned_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Model, Initialize DataPipeline\n",
    "\n",
    "The next section will initialize the model and data pipeline for the quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the pipeline and the model. Before quantizing the model, we calculate the original floating point (FP32) accuracy of the model on the dataset provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipeline = ImageNetDataPipeline(config)\n",
    "\n",
    "model = resnet18(pretrained=True)\n",
    "if config['use_cuda']:\n",
    "    if torch.cuda.is_available():\n",
    "        model.to(torch.device('cuda'))\n",
    "    else:\n",
    "        raise Exception(\"use_cuda is True but cuda is unavailable\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quantization Simulator\n",
    "\n",
    "The next cells are for the actual quantization step. The quantization parameters are specified in the following cell:\n",
    "\n",
    "1. **quant_scheme**: The scheme used to quantize the model. We can choose from s - post_training_tf or post_training_tf_enhanced.\n",
    "\n",
    "2. **rounding_mode**: The rounding mode used for quantization. There are two possible choices here - 'nearest' or 'stochastic'\n",
    "\n",
    "3. **default_output_bw**: The bitwidth of the activation tensors. The value of this should be a power of 2, less than 32.\n",
    "\n",
    "4. **default_param_bw**: The bidwidth of the parameter tensors. The value of this should be a power of 2, less than 32.\n",
    "\n",
    "5. **num_batches**: The number of batches used to evaluate the model while calculating the quantization encodings.Number of batches to use for computing encodings. Only 5 batches are used here to speed up the process. In addition, the number of images in these 5 batches should be sufficient for compute encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_scheme = QuantScheme.post_training_tf_enhanced\n",
    "rounding_mode = 'nearest'\n",
    "default_output_bw = 8\n",
    "default_param_bw = 8\n",
    "\n",
    "#Uncomment one of the following lines\n",
    "# num_batches = 5 #Typical\n",
    "num_batches = 1 #Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Calculate floating point accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = data_pipeline.evaluate(model, use_cuda=config['use_cuda'])\n",
    "print(\"Original Model Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Calculate Quant Simulator accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set up the quantization simulator, and quantize the model. The resulting quantized (INT8) Model is then evaluated on the dataset. We utilize the evaluate function from the data pipeline to compute the new weights.\n",
    "\n",
    "it is customary to fold batch norms; however, the Cross Layer Equalization API expects a model which does not have folded batch norms. For this reason, we make a copy of our model to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.rand(1, 3, 224, 224)\n",
    "if config['use_cuda']:\n",
    "    dummy_input = dummy_input.to(torch.device('cuda'))\n",
    "\n",
    "\n",
    "BN_folded_model = copy.deepcopy(model)\n",
    "_ = fold_all_batch_norms(BN_folded_model, input_shapes=(1, 3, 224, 224))\n",
    "\n",
    "quantizer = QuantizationSimModel(model=BN_folded_model,\n",
    "                                 quant_scheme=quant_scheme,\n",
    "                                 dummy_input=dummy_input,\n",
    "                                 rounding_mode=rounding_mode,\n",
    "                                 default_output_bw=default_output_bw,\n",
    "                                 default_param_bw=default_param_bw)\n",
    "\n",
    "quantizer.compute_encodings(forward_pass_callback=partial(data_pipeline.evaluate,\n",
    "                                                          use_cuda=config['use_cuda']),\n",
    "                            forward_pass_callback_args=num_batches)\n",
    "\n",
    "# Calculate quantized (INT8) accuracy after CLE\n",
    "accuracy = data_pipeline.evaluate(quantizer.model)\n",
    "print(\"Quantized (INT8) Model Top-1 Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apply AIMET CLE and BC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 1 Cross Layer Equalization\n",
    "\n",
    "The next cell performs cross-layer equalization on the model. As noted before, the function folds batch norms, applies cross-layer scaling, and then folds high biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This API will equalize the model in-place\n",
    "equalize_model(model, input_shapes=(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the model is quantized, and the accuracy is noted. This is done before the bias correction step in order to measure the individual impacts of each technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.rand(1, 3, 224, 224)\n",
    "if config['use_cuda']:\n",
    "    dummy_input = dummy_input.to(torch.device('cuda'))\n",
    "\n",
    "cle_quantizer = QuantizationSimModel(model=model,\n",
    "                                     quant_scheme=quant_scheme,\n",
    "                                     dummy_input=dummy_input,\n",
    "                                     rounding_mode=rounding_mode,\n",
    "                                     default_output_bw=default_output_bw,\n",
    "                                     default_param_bw=default_param_bw)\n",
    "\n",
    "cle_quantizer.compute_encodings(forward_pass_callback=partial(data_pipeline.evaluate,\n",
    "                                                              use_cuda=config['use_cuda']),\n",
    "                                forward_pass_callback_args=num_batches)\n",
    "\n",
    "accuracy = data_pipeline.evaluate(cle_quantizer.model)\n",
    "print(\"CLE applied Model Top-1 accuracy on Quant Simulator: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 2 Bias Correction\n",
    "\n",
    "Perform Bias correction and calculate the accuracy on the quantsim model. The first cell includes two parameters related to this step:\n",
    "\n",
    "1. **num_quant_samples**: The number of samples used during quantization\n",
    "2. **num_bias_correction_samples**: The number of samples used during bias correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment one of the following sets of parameters\n",
    "num_quant_samples = 16 #Typical\n",
    "num_bias_correct_samples = 16 #Typical\n",
    "\n",
    "num_quant_samples = 1 #Test\n",
    "num_bias_correct_samples = 1 #Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the actual bias correction steps are performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = data_pipeline.data_loader()\n",
    "\n",
    "bc_params = QuantParams(weight_bw=default_param_bw,\n",
    "                        act_bw=default_output_bw,\n",
    "                        round_mode=rounding_mode,\n",
    "                        quant_scheme=quant_scheme)\n",
    "\n",
    "correct_bias(model,\n",
    "             bc_params,\n",
    "             num_quant_samples=num_quant_samples,\n",
    "             data_loader=data_loader,\n",
    "             num_bias_correct_samples=num_bias_correct_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the model is quantized, the accuracy is logged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.rand(1, 3, 224, 224)\n",
    "if config['use_cuda']:\n",
    "    dummy_input = dummy_input.to(torch.device('cuda'))\n",
    "\n",
    "quantsim = QuantizationSimModel(model=model,\n",
    "                                quant_scheme=quant_scheme,\n",
    "                                dummy_input=dummy_input,\n",
    "                                rounding_mode=rounding_mode,\n",
    "                                default_output_bw=default_output_bw,\n",
    "                                default_param_bw=default_param_bw,\n",
    "                                in_place=False)\n",
    "\n",
    "quantsim.compute_encodings(forward_pass_callback=partial(data_pipeline.evaluate,\n",
    "                                                         use_cuda=config['use_cuda']),\n",
    "                               forward_pass_callback_args=num_batches)\n",
    "\n",
    "accuracy = data_pipeline.evaluate(quantsim.model)\n",
    "print(\"Quantized (INT8) Model Top-1 Accuracy After Bias Correction: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fine-tune Model\n",
    "\n",
    "\n",
    "After the model is quantized, the model is finetuned, then evaluated and saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model is fine-tuned for 1 epoch, to get better accuracy, the number of epochs can be increased\n",
    "print(\"Starting Model Finetuning\")\n",
    "data_pipeline.finetune(quantsim.model)\n",
    "\n",
    "# Calculate and log the accuracy of quantized-finetuned model\n",
    "accuracy = data_pipeline.evaluate(quantsim.model, use_cuda=config['use_cuda'])\n",
    "print(\"After Quantization Aware Training, top-1 accuracy = %.2f\", accuracy)\n",
    "\n",
    "print(\"Quantization Aware Training Complete\")\n",
    "\n",
    "input_shape = (1, 3, 224, 224)\n",
    "dummy_input = torch.rand(input_shape)\n",
    "\n",
    "# Save the quantized model\n",
    "quantsim.export(path=config['logdir'], filename_prefix='QAT_resnet', dummy_input=dummy_input.cpu())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
