{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Channel Pruning Example Code\n",
    "\n",
    "The following notebook is an example as to how one may compress their model via Channel Pruning using the AIMET library. The general procedure for compressing is to use AIMET's ModelCompressor, after specifying parameters determining the manner of compression, to compress the model, then finetuning it to recover lost accuracy.\n",
    "\n",
    "We now present an overview of the technique. Recall that in any model, a convolutional layer is defined by four dimensions (m, n, h, w), where m and n are the number of input and output channels, respectively; and h and w are the height and width of the convolutional kernel. Channel Pruning seeks to reduce the number of input channels in this convolutional layer. There are two steps - winnowing, which removes less informative channels, and weight reconstruction, which seeks to shift the weights such that a linear regression between the old outputs and new outputs exists with minimal error.\n",
    "\n",
    "#### The example code shows the following:\n",
    "1. Instantiate Data Pipeline for evaluation\n",
    "2. Load the pretrained resnet18 Pytorch model and get starting accuracy\n",
    "3. Compress using channel pruning and obtain resulting accuracy\n",
    "4. Finetune and obtain final accuracy\n",
    "\n",
    "The first three cells below take care of all necessary imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \".*param.*\")\n",
    "\n",
    "# Imports necessary for the notebook\n",
    "import os\n",
    "from typing import Tuple\n",
    "from datetime import datetime\n",
    "from decimal import Decimal\n",
    "import torch\n",
    "from torchvision.models import resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIMET Imports\n",
    "from aimet_torch.compress import ModelCompressor\n",
    "from aimet_common.defs import CompressionScheme, CostMetric\n",
    "from aimet_torch.defs import GreedySelectionParameters, ChannelPruningParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports needed for the Data Pipeline\n",
    "from Examples.common import image_net_config\n",
    "from Examples.torch.utils.image_net_data_loader import ImageNetDataLoader\n",
    "from Examples.torch.utils.image_net_evaluator import ImageNetEvaluator\n",
    "from Examples.torch.utils.image_net_trainer import ImageNetTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Our Config Dictionary\n",
    "\n",
    "The config dictionary specifies a number of things for use in the pipeline:\n",
    "* dataset_dir: Path to a directory containing ImageNet dataset. This folder should contain at least 2 subfolders: 'train': for training dataset and 'val': for validation dataset.\n",
    "* use_cuda: A boolean var to indicate to run the test on GPU.\n",
    "* logdir: Path to a directory for logging.\n",
    "* epochs: Number of epochs to be used in fine tuning.\n",
    "* learning_rate: A float type learning rate for model finetuning.\n",
    "* learning_rate_schedule: A list of epoch indices for learning rate schedule used in finetuning. Check https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html#MultiStepLR for more details.\n",
    "\n",
    "The config dictionary is used for all of the remaining cells. To get a better understanding of when each of the parameters in the config dictionary is used, refer to the code in those cells.\n",
    "\n",
    "**Note**: You will have to replace the dataset_dir path with the path to your own imagenet/tinyimagenet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'dataset_dir': \"path/to/dataset\", # Replace with the directory of your dataset!\n",
    "          'use_cuda': True,\n",
    "          'logdir': os.path.join(\"benchmark_output\", \"channel_pruning_\"+datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")),\n",
    "          'epochs': 15,\n",
    "          'learning_rate': 1e-2,\n",
    "          'learning_rate_schedule': [5, 10]\n",
    "         }\n",
    "\n",
    "os.makedirs(config['logdir'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instantiate Data Pipeline\n",
    "\n",
    "The next cell defines the data pipeline. The ImageNetDataPipeline class takes care of both evaluating and finetuning a model using a dataset directory (which should contain both training data and validation data, already separated into folders) that is specified by the user. For more detail on how it works, see the relevant files under examples/torch/utils.\n",
    "\n",
    "The data pipeline class is simply a template for the user to follow. The methods for this class can be replaced by the user to fit their needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetDataPipeline:\n",
    "    \"\"\"\n",
    "    Provides APIs for model quantization using evaluation and finetuning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        :param config:\n",
    "        \"\"\"\n",
    "        self._config = config\n",
    "\n",
    "    def data_loader(self):\n",
    "        \"\"\"\n",
    "        :return: ImageNetDataloader\n",
    "        \"\"\"\n",
    "        \n",
    "        data_loader = ImageNetDataLoader(is_training=False, images_dir=self._config[\"dataset_dir\"],\n",
    "                                         image_size=image_net_config.dataset['image_size']).data_loader\n",
    "\n",
    "        return data_loader\n",
    "    \n",
    "    def evaluate(self, model: torch.nn.Module, iterations: int = None, use_cuda: bool = False) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate the specified model using the specified number of samples from the validation set.\n",
    "        :param model: The model to be evaluated.\n",
    "        :param iterations: The number of batches of the dataset.\n",
    "        :param use_cuda: If True then use a GPU for inference.\n",
    "        :return: The accuracy for the sample with the maximum accuracy.\n",
    "        \"\"\"\n",
    "\n",
    "        # Your code goes here\n",
    "\n",
    "        evaluator = ImageNetEvaluator(self._config['dataset_dir'], image_size=image_net_config.dataset['image_size'],\n",
    "                                      batch_size=image_net_config.evaluation['batch_size'],\n",
    "                                      num_workers=image_net_config.evaluation['num_workers'])\n",
    "\n",
    "        return evaluator.evaluate(model, iterations, use_cuda)\n",
    "    \n",
    "    def finetune(self, model: torch.nn.Module):\n",
    "        \"\"\"\n",
    "        Finetunes the model.  The implemtation provided here is just an example,\n",
    "        provide your own implementation if needed.\n",
    "        :param model: The model to finetune.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "\n",
    "        # Your code goes here instead of the example from below\n",
    "\n",
    "        trainer = ImageNetTrainer(self._config['dataset_dir'], image_size=image_net_config.dataset['image_size'],\n",
    "                                  batch_size=image_net_config.train['batch_size'],\n",
    "                                  num_workers=image_net_config.train['num_workers'])\n",
    "\n",
    "        trainer.train(model, max_epochs=self._config['epochs'], learning_rate=self._config['learning_rate'],\n",
    "                      learning_rate_schedule=self._config['learning_rate_schedule'], use_cuda=self._config['use_cuda'])\n",
    "\n",
    "        torch.save(model, os.path.join(self._config['logdir'], 'finetuned_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Model, Initialize Data Pipeline, Get Starting Accuracy\n",
    "\n",
    "The next section will initialize the model and the data pipeline. It is also customary to log the original accuracy of the model on the dataset provided before compressing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipeline = ImageNetDataPipeline(config)\n",
    "\n",
    "# Input image shape\n",
    "image_shape = (1, image_net_config.dataset['image_channels'],\n",
    "               image_net_config.dataset['image_width'], image_net_config.dataset['image_height'])\n",
    "\n",
    "model = resnet18(pretrained=True)\n",
    "if config['use_cuda']:\n",
    "    if torch.cuda.is_available():\n",
    "        model.to(torch.device('cuda'))\n",
    "    else:\n",
    "        raise Exception(\"use_cuda is True but cuda is unavailable\")\n",
    "\n",
    "accuracy = data_pipeline.evaluate(model, use_cuda=config['use_cuda'])\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compress with Channel Pruning\n",
    "\n",
    "The next cells perform the actual compression step. First, parameters related to the compression are specified in the following cell:\n",
    "\n",
    "1. **target_comp_ratio**: The desired compession ratio using Channel Pruning. This value denotes the desired compression % of the original model. To compress the model to 20% of its original size, use 0.2. This would compress the model by 80%. The pre-specified value that is given is 50%\n",
    "\n",
    "2. **num_comp_ratio_candidates**: The number of compression ratios used by the API at each layer. Note that the model will test multiple different compression ratios per layer to try to compress less-important layers more, in such a way such that the overall compression ratio is equal to target_comp_ratio. The specified value is 10, which means that for each layer, the API will try the values 0.1, 0.2, ... 1.0 as ratios.\n",
    "\n",
    "3. **cost_metric**: Determines in what way the model is evaluated - can either be compute (mac), or space (memory).\n",
    "\n",
    "4. **eval_iterations**: The number of batches of data used to evaluate a model while the model is compressing. It is set to 10 to speed up the compression, rather than using the whole dataset. More details are later in the notebook/elsewhere in the AIMET API documentation\n",
    "\n",
    "5. **modules_to_ignore**: The layers that should be ignored during compression. The first layer is ignored to preserve the way the input interacts with the model; if there are other layers that should be ignored, add them to the list.\n",
    "\n",
    "6. **num_reconstruction_samples**: During the last stage of Channel Pruning, the Compression API tries to map the outputs of the pruned model with that of the original model through linear regression, and uses this attempt to change the weights in the pruned layer. The regression is done with this many random samples. This should generally be in the 100s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_comp_ratio = Decimal(0.5)\n",
    "\n",
    "num_comp_ratio_candidates = 10\n",
    "\n",
    "cost_metric = CostMetric.mac\n",
    "\n",
    "num_eval_iterations = 10\n",
    "\n",
    "modules_to_ignore = [model.conv1]\n",
    "\n",
    "num_reconstruction_samples = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell sets up the other parameters needed to perform the compression.\n",
    "\n",
    "We first define the actual Channel Pruning Parameters. There are two methods for which you can choose parameters - Auto and Manual. For Auto, the only option is a greedy selection scheme, where the optimal compression ratio is selected for each layer among a set list of candidates to reach the target ratio (which was specified in the previous cell). For Manual, you have to specify the compression ratios for each layer; a general rule of thumb, if one is to use Manual, is to start with the ratios found by Auto Mode and use it as a starting point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Data Loader\n",
    "data_loader = ImageNetDataLoader(is_training=True, images_dir=config['dataset_dir'],\n",
    "                                 image_size=image_net_config.dataset['image_size']).data_loader\n",
    "\n",
    "# Creating Greedy selection parameters:\n",
    "greedy_params = GreedySelectionParameters(target_comp_ratio=target_comp_ratio,\n",
    "                                          num_comp_ratio_candidates=num_comp_ratio_candidates)\n",
    "\n",
    "\n",
    "\n",
    "# Creating Auto mode Parameters:\n",
    "cp_mode = ChannelPruningParameters.Mode.auto\n",
    "\n",
    "auto_params = ChannelPruningParameters.AutoModeParams(greedy_select_params=greedy_params, \n",
    "                                                      modules_to_ignore=modules_to_ignore)\n",
    "\n",
    "# Creating Channel Pruning SVD parameters with Auto Mode:\n",
    "params = ChannelPruningParameters(data_loader=data_loader,\n",
    "                                  num_reconstruction_samples=num_reconstruction_samples,\n",
    "                                  allow_custom_downsample_ops=True,\n",
    "                                  mode=cp_mode,\n",
    "                                  params=auto_params)\n",
    "\n",
    "# Scheme is Channel Pruning:\n",
    "scheme = CompressionScheme.channel_pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the model is compressed using AIMET's ModelCompressor paired with the parameters specified above. This returns both the new model, which is saved, as well as relevant statistics. Finally, the compressed model is evaluated on the dataset. Note here that the ModelCompressor evaluates the model while compressing using the same evaluate function that is in our data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_model, comp_stats = ModelCompressor.compress_model(model=model,\n",
    "                                                              eval_callback=data_pipeline.evaluate,\n",
    "                                                              eval_iterations=num_eval_iterations,\n",
    "                                                              input_shape=image_shape,\n",
    "                                                              compress_scheme=scheme,\n",
    "                                                              cost_metric=cost_metric,\n",
    "                                                              parameters=params)\n",
    "\n",
    "torch.save(compressed_model, os.path.join(config['logdir'], 'compressed_model.pth'))\n",
    "\n",
    "print(comp_stats)\n",
    "\n",
    "comp_accuracy = data_pipeline.evaluate(model, use_cuda=config['use_cuda'])\n",
    "print(comp_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Finetuning\n",
    "\n",
    "After the model is compressed, the model is finetuned, then evaluated and saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipeline.finetune(compressed_model)\n",
    "\n",
    "finetuned_accuracy = data_pipeline.evaluate(compressed_model, use_cuda=config['use_cuda'])\n",
    "print(finetuned_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
