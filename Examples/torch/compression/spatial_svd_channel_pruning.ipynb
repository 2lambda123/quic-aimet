{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Spatial SVD and Channel Pruning Example \n",
    "\n",
    "This notebook demonstrates the use of AIMET to apply Spatial SVD and Channel Pruning features on a given model. The general procedure for compressing is to use AIMET's ModelCompressor, after specifying parameters determining the manner of compression, to compress the model,  then fine-tuning it to recover lost accuracy.\n",
    "\n",
    "Let's begin with a brief overview of the techniques used :\n",
    " ### Spatial SVD  \n",
    " Recall that in any model, a convolutional layer is defined by four dimensions (m, n, h, w), where m and n are the number of input and output channels, respectively; and h and w are the height and width of the convolution kernel.\n",
    "Spatial SVD (where SVD stands for Singular Value Decomposition) seeks to split this convolutional layer into two layers of size (m, k, h, 1) and (k, n, 1, w), where k is a parameter that is known as the rank. The weights of the new layers are chosen so that the outputs of the two layers in succession are as similar as possible to the output of the original layer.\n",
    "\n",
    "\n",
    " ### Channel Pruning  \n",
    "Channel Pruning seeks to reduce the number of input channels in this convolutional layer. There are two steps involved: \n",
    "1. Winnowing, which removes less informative channels, and \n",
    "2. Weight reconstruction, which seeks to shift the weights such that a linear regression between the old outputs and new outputs exists with minimal error.\n",
    "\n",
    "### Steps: \n",
    "1. Instantiate Data Pipeline for evaluation\n",
    "2. Load the pre-trained resnet18 PyTorch model and get the original floating point accuracy\n",
    "3. Compress the model and fine-tune: \n",
    "    * 3.1. Compress using Spatial SVD and obtain resulting accuracy\n",
    "    * 3.2. Fine-tune model after Spatial SVD\n",
    "    * 3.3. Compress Spatial SVD compressed model using Channel Pruning and obtain resulting accuracy\n",
    "    * 3.4. Fine-tune model after Channel Pruning\n",
    "\n",
    "### What this notebook is not \n",
    "* This notebook is not designed to show state-of-the-art compression results. Parameters used in this example are chosen such that the example runs quickly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    " The next three cells take care of all the imports necessary for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \".*param.*\")\n",
    "\n",
    "import os\n",
    "from typing import Tuple\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from torchvision.models import resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIMET Imports for Compression\n",
    "from aimet_torch.compress import ModelCompressor\n",
    "from aimet_common.defs import CompressionScheme, CostMetric\n",
    "from aimet_torch.defs import GreedySelectionParameters, SpatialSvdParameters, ChannelPruningParameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports needed for the Data Pipeline\n",
    "from Examples.common import image_net_config\n",
    "from Examples.torch.utils.image_net_data_loader import ImageNetDataLoader\n",
    "from Examples.torch.utils.image_net_evaluator import ImageNetEvaluator\n",
    "from Examples.torch.utils.image_net_trainer import ImageNetTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Our Config Dictionary\n",
    "\n",
    "The config dictionary specifies a number of things\n",
    "config: This mapping expects following parameters:\n",
    "\n",
    "1. **dataset_dir:** Path to a directory containing ImageNet dataset. This folder should contain subfolders 'train' for training dataset and 'val' for validation dataset.\n",
    "2. **use_cuda:** A boolean var to indicate to run the quantization on GPU.\n",
    "3. **output_dir:** Path to a directory for logging.\n",
    "\n",
    "To get a better understanding of when each of the parameters in the config dictionary is used, please read the code in those cells.\n",
    "**Note:** You will have to replace the dataset_dir path with the path to your own imagenet/tinyimagenet dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'dataset_dir': \"/path\/to\/dataset\", #\"path/to/dataset\" # Replace with the directory of your dataset!\n",
    "          'use_cuda': True,\n",
    "          'output_dir': os.path.join(\"benchmark_output\", \"spatial_svd_\"+datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")),\n",
    "          'epochs': 1, # Typical epochs: 15\n",
    "          'learning_rate': 1e-2, \n",
    "          'learning_rate_schedule': [5, 10]}\n",
    "\n",
    "os.makedirs(config['output_dir'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instantiate Data Pipeline\n",
    "\n",
    " The ImageNetDataPipeline class takes both evaluating a model using dataset directory. For more detail on how it works, see the relevant files under examples/torch/utils.\n",
    "\n",
    "The data pipeline class is simply a template for the user to follow. The methods for this class can be replaced by the user to fit their needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetDataPipeline:\n",
    "    \"\"\"\n",
    "    Provides APIs for model evaluation and fine-tuning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        :param _config: a dictionary containing the values of necessary parameters.\n",
    "        \"\"\"\n",
    "        self._config = config\n",
    "        \n",
    "    def data_loader(self):\n",
    "        \"\"\"\n",
    "        :return: ImageNetDataloader\n",
    "        \"\"\"\n",
    "        \n",
    "        data_loader = ImageNetDataLoader(is_training=False, images_dir=self._config[\"dataset_dir\"],\n",
    "                                         image_size=image_net_config.dataset['image_size']).data_loader\n",
    "\n",
    "        return data_loader\n",
    "    \n",
    "    def evaluate(self, model: torch.nn.Module, iterations: int=None, use_cuda: bool=False) -> float:\n",
    "        \"\"\"\n",
    "        Given a torch model, evaluates its Top-1 accuracy on the dataset.\n",
    "        :param model: the model to evaluate.\n",
    "        :param iterations: the number of batches of dataset.\n",
    "        :param use_cuda: If True then use a GPU for inference.\n",
    "        :return: The accuracy for the sample with the maximum accuracy.\n",
    "        \"\"\"\n",
    "        # Your code goes here instead of the example from below\n",
    "        \n",
    "        evaluator = ImageNetEvaluator(self._config['dataset_dir'], image_size=image_net_config.dataset['image_size'],\n",
    "                                      batch_size=image_net_config.evaluation['batch_size'],\n",
    "                                      num_workers=image_net_config.evaluation['num_workers'])\n",
    "        \n",
    "        return evaluator.evaluate(model, iterations, use_cuda)\n",
    "    \n",
    "    def finetune(self, model: torch.nn.Module, modifier: str=\"\"):\n",
    "        \"\"\"\n",
    "        Given a torch model, fine-tunes the model to improve its accuracy.\n",
    "        :param model: the model to fine-tune.\n",
    "        :param modifier: a string that is used to change the name of the path where the model will be saved.\n",
    "        \"\"\"\n",
    "        # Your code goes here instead of the example from below\n",
    "        \n",
    "        trainer = ImageNetTrainer(self._config['dataset_dir'], image_size=image_net_config.dataset['image_size'],\n",
    "                                  batch_size=image_net_config.train['batch_size'],\n",
    "                                  num_workers=image_net_config.train['num_workers'])\n",
    "        \n",
    "        trainer.train(model, max_epochs=self._config['epochs'], learning_rate=self._config['learning_rate'],\n",
    "                      learning_rate_schedule=self._config['learning_rate_schedule'], use_cuda=self._config['use_cuda'])\n",
    "        \n",
    "        torch.save(model, os.path.join(self._config['output_dir'], modifier+'finetuned_model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Model, Initialize DataPipeline, Get Starting Accuracy \n",
    "The next section will initialize the model and data pipeline for compression.\n",
    "We initialize the model and the pipeline calculate the original floating point(FP32) accuracy of the model on the dataset provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipeline = ImageNetDataPipeline(config)\n",
    "\n",
    "# Input image shape (1, num_channels (RGB), image_width, image_height)\n",
    "image_shape = (1, 3, 224, 224)\n",
    "\n",
    "model = resnet18(pretrained=True)\n",
    "if config['use_cuda']:\n",
    "    if torch.cuda.is_available():\n",
    "        model.to(torch.device('cuda'))\n",
    "    else:\n",
    "        raise Exception(\"use_cuda is True but cuda is unavailable\")\n",
    "\n",
    "accuracy = data_pipeline.evaluate(model, use_cuda=config['use_cuda'])\n",
    "print(\"Original Model Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compress the model and fine-tune\n",
    "\n",
    "### 3.1. Compress the model using Spatial SVD \n",
    "The parameters related to the Spatial SVD compression are defined below and initialized in the next cell:\n",
    "\n",
    "1. **ssvd_target_comp_ratio**: The desired compression ratio using Spatial SVD. This value denotes the desired compression % of the original model. To compress the model to 20% of its original size, use 0.2. This would compress the model by 80%. The pre-specified value that is given is 50%.\n",
    "\n",
    "2. **ssvd_num_comp_ratio_candidates**: The number of compression ratios used by the API at each layer. Note that the model will test multiple different compression ratios per layer to try to compress less-important layers more, in such a way such that the overall compression ratio is equal to target_comp_ratio. The specified value is 10, which means that for each layer, the API will try the values 0.1, 0.2, ... 1.0 as ratios.\n",
    "\n",
    "3. **ssvd_cost_metric**: Determines in what way the model is evaluated - can either be compute (mac), or space (memory).\n",
    "\n",
    "4. **ssvd_eval_iterations**: The number of batches of data used to evaluate a model while the model is compressing. It is set to 10 to speed up the compression, rather than using the whole dataset.\n",
    "\n",
    "5. **ssvd_modules_to_ignore**: The layers that should be ignored during compression. The first layer is ignored to preserve the way the input interacts with the model; if there are other layers that should be ignored, they can be added to this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssvd_target_comp_ratio = Decimal(0.5)\n",
    "\n",
    "# ssvd_num_comp_ratio_candidates = 10     # Typical\n",
    "ssvd_num_comp_ratio_candidates = 2\n",
    "\n",
    "ssvd_cost_metric = CostMetric.mac\n",
    "\n",
    "# ssvd_num_eval_iterations = 10     # Typical\n",
    "ssvd_num_eval_iterations = 1\n",
    "\n",
    "ssvd_modules_to_ignore = [model.conv1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell sets up the other parameters needed to perform this part of the compression.\n",
    "\n",
    " There are two methods for which you can choose parameters: \n",
    "\n",
    "a. **Auto:** \n",
    "This mode supports only greedy selection scheme, where the optimal compression ratio is selected for each layer among a set list of candidates to \n",
    "reach the target ratio.\n",
    "\n",
    "b. **Manual:** \n",
    " In this mode, the compression ratios for each layer are to be specified by the user. A general rule of \n",
    " thumb is to use the ratios found by Auto Mode as a starting point for this mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ssvd_greedy_params = GreedySelectionParameters(target_comp_ratio=ssvd_target_comp_ratio,\n",
    "                                               num_comp_ratio_candidates=ssvd_num_comp_ratio_candidates)\n",
    "\n",
    "ssvd_auto_params = SpatialSvdParameters.AutoModeParams(greedy_select_params=ssvd_greedy_params,\n",
    "                                                       modules_to_ignore=ssvd_modules_to_ignore)\n",
    "\n",
    "ssvd_params = SpatialSvdParameters(SpatialSvdParameters.Mode.auto, ssvd_auto_params)\n",
    "\n",
    "ssvd_scheme = CompressionScheme.spatial_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the model is compressed using AIMET's ModelCompressor paired with the parameters specified above. This returns both the new model, which is saved, as well as relevant statistics. Finally, the compressed model is evaluated on the dataset. Note here that the ModelCompressor evaluates the model while compressing using the same evaluate function that is in our data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssvd_compressed_model, ssvd_comp_stats = ModelCompressor.compress_model(model=model,\n",
    "                                                                        eval_callback=data_pipeline.evaluate,\n",
    "                                                                        eval_iterations=ssvd_num_eval_iterations,\n",
    "                                                                        input_shape=image_shape,\n",
    "                                                                        compress_scheme=ssvd_scheme,\n",
    "                                                                        cost_metric=ssvd_cost_metric,\n",
    "                                                                        parameters=ssvd_params)\n",
    "\n",
    "print(ssvd_comp_stats)\n",
    "\n",
    "ssvd_comp_accuracy = data_pipeline.evaluate(ssvd_compressed_model, use_cuda=config['use_cuda'])\n",
    "print(\"Accuracy of Spatial SVD compressed Model: \", ssvd_comp_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Fine-tune model after Spatial SVD \n",
    "\n",
    "After the model is compressed through Spatial SVD, the model is fine-tuned, evaluated and saved. It is customary to do two rounds of fine-tuning so that model accuracy is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipeline.finetune(ssvd_compressed_model)\n",
    "\n",
    "ssvd_finetuned_accuracy = data_pipeline.evaluate(ssvd_compressed_model, use_cuda=config['use_cuda'])\n",
    "print(\"Spatial SVD compressed Model Accuracy after fine-tuning: \", ssvd_finetuned_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Compress Spatial SVD compressed model using Channel Pruning\n",
    "\n",
    "The fine-tuned model, compressed with Spatial SVD is now compressed using Channel pruning.",
    "The next two cells specify the parameters for compression using Channel Pruning:\n",
    "\n",
    "1. **cp_target_comp_ratio**: The desired compression ratio using Channel Pruning. This value denotes the desired compression % of the original model. To compress the model to 20% of its original size, use 0.2. This would compress the model by 80%. The pre-specified value that is given is 66% of its original size.\n",
    "\n",
    "2. **cp_num_comp_ratio_candidates**: The number of compression ratios used by the API at each layer. Note that the model will test multiple different compression ratios per layer to try to compress less-important layers more, in such a way such that the overall compression ratio is equal to target_comp_ratio. The specified value is 10, which means that for each layer, the API will try the values 0.1, 0.2, ... 1.0 as ratios.\n",
    "\n",
    "3. **cp_cost_metric**: Determines in what way the model is evaluated - can either be compute (mac), or space (memory).\n",
    "\n",
    "4. **cp_eval_iterations**: The number of batches of data used to evaluate a model while the model is compressing. It is set to 10 to speed up the compression, rather than using the whole dataset. More details are later in the notebook/elsewhere in the AIMET API documentation\n",
    "\n",
    "5. **cp_modules_to_ignore**: The layers that should be ignored during compression. The first layer is ignored to preserve the way the input interacts with the model; if there are other layers that should be ignored, add them to the list.\n",
    "\n",
    "6. **num_reconstruction_samples**: During the last stage of Channel Pruning, the Compression API tries to map the outputs of the pruned model with that of the original model through linear regression, and uses this attempt to change the weights in the pruned layer. The regression is done with this many random samples. This should generally be in the 100s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_target_comp_ratio = Decimal(0.66)\n",
    "\n",
    "# cp_num_comp_ratio_candidates = 10     # Typical\n",
    "cp_num_comp_ratio_candidates = 2\n",
    "\n",
    "cp_cost_metric = CostMetric.mac\n",
    "\n",
    "# cp_num_eval_iterations = 10     # Typical\n",
    "cp_num_eval_iterations = 1\n",
    "\n",
    "cp_modules_to_ignore = [ssvd_compressed_model.conv1]\n",
    "\n",
    "num_reconstruction_samples = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Spatial SVD, there are two methods for which you can choose parameters - Auto and Manual, and they have the same general meanings specified under Spatial SVD section above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_loader = data_pipeline.data_loader()\n",
    "\n",
    "cp_greedy_params = GreedySelectionParameters(target_comp_ratio=cp_target_comp_ratio,\n",
    "                                             num_comp_ratio_candidates=cp_num_comp_ratio_candidates)\n",
    "\n",
    "\n",
    "\n",
    "cp_mode = ChannelPruningParameters.Mode.auto\n",
    "\n",
    "cp_auto_params = ChannelPruningParameters.AutoModeParams(greedy_select_params=cp_greedy_params, \n",
    "                                                         modules_to_ignore=cp_modules_to_ignore)\n",
    "\n",
    "cp_params = ChannelPruningParameters(data_loader=data_loader,\n",
    "                                     num_reconstruction_samples=num_reconstruction_samples,\n",
    "                                     allow_custom_downsample_ops=True,\n",
    "                                     mode=cp_mode,\n",
    "                                     params=cp_auto_params)\n",
    "\n",
    "cp_scheme = CompressionScheme.channel_pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we call ModelCompressor to perform the actual model compression, and then evaluate the compressed model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssvd_cp_compressed_model, cp_comp_stats = ModelCompressor.compress_model(model=ssvd_compressed_model,\n",
    "                                                                         eval_callback=data_pipeline.evaluate,\n",
    "                                                                         eval_iterations=cp_num_eval_iterations,\n",
    "                                                                         input_shape=image_shape,\n",
    "                                                                         compress_scheme=cp_scheme,\n",
    "                                                                         cost_metric=cp_cost_metric,\n",
    "                                                                         parameters=cp_params)\n",
    "\n",
    "print(cp_comp_stats)\n",
    "\n",
    "ssvd_cp_comp_accuracy = data_pipeline.evaluate(ssvd_cp_compressed_model, use_cuda=config['use_cuda'])\n",
    "print(\"Accuracy of Model compressed using Spatial SVD and Channel Pruning: \", ssvd_cp_comp_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Fine-tune model after Channel Pruning \n",
    "\n",
    "After the model is compressed through Channel Pruning, the model is fine-tuned, then evaluated and saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipeline.finetune(ssvd_cp_compressed_model, modifier=\"channel_pruned_\")\n",
    "\n",
    "ssvd_cp_finetuned_accuracy = data_pipeline.evaluate(ssvd_compressed_model, use_cuda=config['use_cuda'])\n",
    "print(\"Accurracy of Model compressed using Spatial SVD and Channel Pruning, after fine-tuning : \", ssvd_cp_finetuned_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example illustrated how AIMET Spatial SVD and Channel Pruning can be used together to achieve model compression.\n",
    "\n",
    "To use AIMET for your specific needs, replace the model with your model and replace the Data pipeline with your data pipeline. This will provide you a quick starting point.\n",
    "\n",
    "As indicated above, some parameters have been chosen in a way to run the example faster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
