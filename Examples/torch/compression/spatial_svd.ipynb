{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Spatial SVD Notebook\n",
    "\n",
    "The following notebook is an example as to how one may compress their model via Spatial SVD using the AIMET library. The general procedure for compressing is to use AIMET's ModelCompressor, after specifying parameters determining the manner of compression, to compress the model, then finetuning it to recover lost accuracy.\n",
    "\n",
    "We now present an overview of the technique. Recall that in any model, a convolutional layer is defined by four dimensions (m, n, h, w), where m and n are the number of input and output channels, respectively; and h and w are the height and width of the convolutional kernel. Spatial SVD (where SVD stands for Singular Value Decomposition) seeks to split this convolutional layer into two smaller layers of size (m, k, h, 1) and (k, n, 1, w), where k is a parameter that is known as the rank. The weights of the new layers are chosen so that the outputs of the two layers in succession are as similar as possible to the output of the original layer.\n",
    "\n",
    "The first three cells below takes care of all necessary imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \".*param.*\")\n",
    "\n",
    "# Imports necessary for the notebook\n",
    "import os\n",
    "from typing import Tuple\n",
    "from datetime import datetime\n",
    "from decimal import Decimal\n",
    "import torch\n",
    "from torchvision.models import resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIMET Imports for Compression\n",
    "from aimet_torch.compress import ModelCompressor\n",
    "from aimet_common.defs import CompressionScheme, CostMetric\n",
    "from aimet_torch.defs import GreedySelectionParameters, SpatialSvdParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports needed for the Data Pipeline\n",
    "from Examples.common import image_net_config\n",
    "from Examples.torch.utils.image_net_evaluator import ImageNetEvaluator\n",
    "from Examples.torch.utils.image_net_trainer import ImageNetTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "config: This dictionary expects the following parameters:\n",
    "             1. dataset_dir: Path to a directory containing ImageNet dataset. This folder should contain at least \n",
    "                             2 subfolders: 'train': for training dataset and 'val': for validation dataset.\n",
    "             2. use_cuda: A boolean var to indicate to run the test on GPU.\n",
    "             3. logdir: Path to a directory for logging.\n",
    "\n",
    "The config dictionary is used for all of the remaining cells. To get a better understanding of when each of the parameters in the config dictionary is used, read the code in those cells.\n",
    "\n",
    "Explanation for few of the important parameters are below. We encourage the user to change them to suit their needs.\n",
    "\n",
    "1. **epochs**: Number of epochs (type int) for finetuning.\n",
    "2. **learning_rate**: A float type learning rate for model finetuning\n",
    "3. **learning_rate_schedule**: A list of epoch indices for learning rate schedule used in finetuning. Check https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html#MultiStepLR for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'dataset_dir': \"path/to/dataset\", # Replace with the directory of your dataset!\n",
    "          'use_cuda': True,\n",
    "          'logdir': os.path.join(\"benchmark_output\", \"spatial_svd_\"+datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")),\n",
    "          'epochs': 1, \n",
    "          'learning_rate': 1e-2, \n",
    "          'learning_rate_schedule': [5, 10]}\n",
    "\n",
    "os.makedirs(config['logdir'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline\n",
    "\n",
    "The next cell is to define the data pipeline. The ImageNetDataPipeline class takes care of both evaluating and finetuning a model using a dataset directory (which should contain both training data and validation data, already separated into folders) that is specified by the user. For more detail on how it works, see the relevant files under examples/torch/utils.\n",
    "\n",
    "The data pipeline class is simply a template for the user to follow. The methods for this class can be replaced by the user to fit their needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetDataPipeline:\n",
    "    \"\"\"\n",
    "    Provides APIs for model compression using AIMET weight SVD, evaluation and finetuning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        :param config:\n",
    "        \"\"\"\n",
    "        self._config = config\n",
    "\n",
    "    \n",
    "    def evaluate(self, model: torch.nn.Module, iterations: int = None, use_cuda: bool = False) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate the specified model using the specified number of samples from the validation set.\n",
    "        AIMET's compress_model() expects the function with this signature to its eval_callback\n",
    "        parameter.\n",
    "\n",
    "        :param model: The model to be evaluated.\n",
    "        :param iterations: The number of batches of the dataset.\n",
    "        :param use_cuda: If True then use a GPU for inference.\n",
    "        :return: The accuracy for the sample with the maximum accuracy.\n",
    "        \"\"\"\n",
    "\n",
    "        # your code goes here instead of the example from below\n",
    "\n",
    "        evaluator = ImageNetEvaluator(self._config['dataset_dir'], image_size=image_net_config.dataset['image_size'],\n",
    "                                      batch_size=image_net_config.evaluation['batch_size'],\n",
    "                                      num_workers=image_net_config.evaluation['num_workers'])\n",
    "\n",
    "        return evaluator.evaluate(model, iterations, use_cuda)\n",
    "    \n",
    "    def finetune(self, model: torch.nn.Module):\n",
    "        \"\"\"\n",
    "        Finetunes the model.  The implemtation provided here is just an example,\n",
    "        provide your own implementation if needed.\n",
    "\n",
    "        :param model: The model to finetune.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "\n",
    "        # Your code goes here instead of the example from below\n",
    "\n",
    "        trainer = ImageNetTrainer(self._config['dataset_dir'], image_size=image_net_config.dataset['image_size'],\n",
    "                                  batch_size=image_net_config.train['batch_size'],\n",
    "                                  num_workers=image_net_config.train['num_workers'])\n",
    "\n",
    "        trainer.train(model, max_epochs=self._config['epochs'], learning_rate=self._config['learning_rate'],\n",
    "                      learning_rate_schedule=self._config['learning_rate_schedule'], use_cuda=self._config['use_cuda'])\n",
    "\n",
    "        torch.save(model, os.path.join(self._config['logdir'], 'finetuned_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next initialize the pipeline and the model. Before compressing the model, it is customary to log the original accuracy of the model on the dataset provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipeline = ImageNetDataPipeline(config)\n",
    "\n",
    "model = resnet18(pretrained=True)\n",
    "if config['use_cuda']:\n",
    "    if torch.cuda.is_available():\n",
    "        model.to(torch.device('cuda'))\n",
    "    else:\n",
    "        raise Exception(\"use_cuda is True but cuda is unavailable\")\n",
    "\n",
    "accuracy = data_pipeline.evaluate(model, use_cuda=config['use_cuda'])\n",
    "print(\"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compression\n",
    "\n",
    "The next cells are for the actual compression step. First, parameters related to the compression are specified in the following cell:\n",
    "\n",
    "1. **target_comp_ratio**: The desired compession ratio using Spatial SVD. This value denotes the desired compression % of the original model. To compress the model to 20% of its original size, use 0.2. This would compress the model by 80%. The pre-specified value that is given is 50%\n",
    "\n",
    "2. **num_comp_ratio_candidates**: The number of compression ratios used by the API at each layer. Note that the model will test multiple different compression ratios per layer to try to compress less-important layers more, in such a way such that the overall compression ratio is equal to target_comp_ratio. The specified value is 10, which means that for each layer, the API will try the values 0.1, 0.2, ... 1.0 as ratios.\n",
    "\n",
    "3. **cost_metric**: Determines in what way the model is evaluated - can either be compute (mac), or space (memory).\n",
    "\n",
    "4. **eval_iterations**: The number of batches of data used to evaluate a model while the model is compressing. It is set to 10 to speed up the compression, rather than using the whole dataset. More details are later in the notebook/elsewhere in the AIMET API documentation\n",
    "\n",
    "5. **modules_to_ignore**: The layers that should be ignored during compression. The first layer is ignored to preserve the way the input interacts with the model; if there are other layers that should be ignored, add them to the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_comp_ratio = Decimal(0.5)\n",
    "\n",
    "# Uncomment one of the following lines\n",
    "# num_comp_ratio_candidates = 10 # Typical\n",
    "num_comp_ratio_candidates = 2 # Test\n",
    "\n",
    "cost_metric = CostMetric.mac\n",
    "\n",
    "# Uncomment one of the following lines\n",
    "# num_eval_iterations = 10 # Typical\n",
    "num_eval_iterations = 1 # Test\n",
    "\n",
    "modules_to_ignore = [model.conv1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell sets up the other parameters needed to perform the compression.\n",
    "\n",
    "The next lines after these create the actual parameters for Spatial SVD. There are two methods for which you can choose parameters - Auto and Manual. For Auto, the only option is a greedy selection scheme, where the optimal compression ratio is selected for each layer among a set list of candidates to reach the target ratio (which was specified in the previous cell). For Manual, you have to specify the compression ratios for each layer; a general rule of thumb, if one is to use Manual, is to start with the ratios found by Auto Mode and use it as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Greedy selection parameters:\n",
    "greedy_params = GreedySelectionParameters(target_comp_ratio=target_comp_ratio,\n",
    "                                          num_comp_ratio_candidates=num_comp_ratio_candidates)\n",
    "\n",
    "# Creating Auto mode Parameters:\n",
    "mode = SpatialSvdParameters.Mode.auto\n",
    "\n",
    "auto_params = SpatialSvdParameters.AutoModeParams(greedy_select_params=greedy_params,\n",
    "                                                  modules_to_ignore=modules_to_ignore)\n",
    "\n",
    "# Creating Spatial SVD parameters with Auto Mode:\n",
    "params = SpatialSvdParameters(mode, auto_params)\n",
    "\n",
    "# Scheme is Spatial SVD:\n",
    "scheme = CompressionScheme.spatial_svd\n",
    "\n",
    "# Input image shape\n",
    "image_shape = (1, image_net_config.dataset['image_channels'],\n",
    "               image_net_config.dataset['image_width'], image_net_config.dataset['image_height'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the model is compressed using AIMET's ModelCompressor paired with the parameters specified above. This returns both the new model, which is saved, as well as relevant statistics. Finally, the compressed model is evaluated on the dataset. Note here that the ModelCompressor evaluates the model while compressing using the same evaluate function that is in our data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_model, comp_stats = ModelCompressor.compress_model(model=model,\n",
    "                                                              eval_callback=data_pipeline.evaluate,\n",
    "                                                              eval_iterations=num_eval_iterations,\n",
    "                                                              input_shape=image_shape,\n",
    "                                                              compress_scheme=scheme,\n",
    "                                                              cost_metric=cost_metric,\n",
    "                                                              parameters=params)\n",
    "\n",
    "torch.save(compressed_model, os.path.join(config['logdir'], 'compressed_model.pth'))\n",
    "\n",
    "print(comp_stats)\n",
    "\n",
    "comp_accuracy = data_pipeline.evaluate(compressed_model, use_cuda=config['use_cuda'])\n",
    "print(comp_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning\n",
    "\n",
    "After the model is compressed, the model is finetuned, then evaluated and saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipeline.finetune(compressed_model)\n",
    "\n",
    "finetuned_accuracy = data_pipeline.evaluate(compressed_model, use_cuda=config['use_cuda'])\n",
    "print(finetuned_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
