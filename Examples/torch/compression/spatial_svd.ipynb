{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Spatial SVD Example\n",
    "\n",
    "The following notebook is an example as to how one may compress their model via Spatial SVD using the AIMET library. The general procedure for compressing is to use AIMET's ModelCompressor, after specifying parameters determining the manner of compression, to compress the model, then finetuning it to recover lost accuracy.\n",
    "\n",
    "Recall that in any model, a convolutional layer is defined by four dimensions (m, n, h, w), where m and n are the number of input and output channels, respectively; and h and w are the height and width of the convolutional kernel. Spatial SVD (where SVD stands for Singular Value Decomposition) seeks to split this convolutional layer into two smaller layers of size (m, k, h, 1) and (k, n, 1, w), where k is a parameter that is known as the rank. The weights of the new layers are chosen so that the outputs of the two layers in succession are as similar as possible to the output of the original layer.\n",
    "\n",
    "#### Steps\n",
    "1. Instantiate data pipeline for evaluation and finetuning\n",
    "2. Perform compression of the model\n",
    "3. Perform finetuning to improve the accuracy\n",
    "\n",
    "#### What this notebook is not\n",
    "* This notebook is not designed to show state-of-the-art Compression results. Parameters used in this example are chosen such that the example runs faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \".*param.*\")\n",
    "\n",
    "# Imports necessary for the notebook\n",
    "import os\n",
    "from typing import Tuple\n",
    "from datetime import datetime\n",
    "from decimal import Decimal\n",
    "import torch\n",
    "from torchvision.models import resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIMET Imports for Compression\n",
    "from aimet_torch.compress import ModelCompressor\n",
    "from aimet_common.defs import CompressionScheme, CostMetric\n",
    "from aimet_torch.defs import GreedySelectionParameters, SpatialSvdParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports needed for the Data Pipeline\n",
    "from Examples.common import image_net_config\n",
    "from Examples.torch.utils.image_net_evaluator import ImageNetEvaluator\n",
    "from Examples.torch.utils.image_net_trainer import ImageNetTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "config: This dictionary expects the following parameters:\n",
    "   1. dataset_dir: Path to a directory containing ImageNet dataset. This folder should contain at least 'train' for training dataset and 'val' for validation dataset\n",
    "   2. use_cuda: A boolean var to indicate to run the test on GPU\n",
    "   3. output_dir: Path to a directory for logging\n",
    "   4. epochs: Number of epochs (type int) for finetuning\n",
    "   5. learning_rate: A float type learning rate for model finetuning\n",
    "   6. learning_rate_schedule: A list of epoch indices for learning rate schedule used in finetuning. Check https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html#MultiStepLR for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'dataset_dir': \"path/to/dataset\", # Replace with the directory of your dataset!\n",
    "          'use_cuda': True,\n",
    "          'output_dir': os.path.join(\"benchmark_output\", \"spatial_svd_\"+datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")),\n",
    "          'epochs': 1, \n",
    "          'learning_rate': 1e-2, \n",
    "          'learning_rate_schedule': [5, 10]}\n",
    "\n",
    "os.makedirs(config['output_dir'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instantiate Data Pipeline\n",
    "\n",
    "The next cell is to define the data pipeline. The ImageNetDataPipeline class takes care of both evaluating and finetuning a model using a dataset directory (which should contain both training data and validation data, already separated into folders) that is specified by the user. For more detail on how it works, see the relevant files under examples/torch/utils.\n",
    "\n",
    "The data pipeline class is simply a template for the user to follow. The methods for this class can be replaced by the user to fit their needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetDataPipeline:\n",
    "    \"\"\"\n",
    "    Provides APIs for model compression using AIMET Spatial SVD, evaluation and finetuning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        :param config:\n",
    "        \"\"\"\n",
    "        self._config = config\n",
    "\n",
    "    \n",
    "    def evaluate(self, model: torch.nn.Module, iterations: int = None, use_cuda: bool = False) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate the specified model using the specified number of samples from the validation set.\n",
    "        AIMET's compress_model() expects the function with this signature to its eval_callback\n",
    "        parameter.\n",
    "\n",
    "        :param model: The model to be evaluated.\n",
    "        :param iterations: The number of batches of the dataset.\n",
    "        :param use_cuda: If True then use a GPU for inference.\n",
    "        :return: The accuracy for the sample with the maximum accuracy.\n",
    "        \"\"\"\n",
    "\n",
    "        # your code goes here instead of the example from below\n",
    "\n",
    "        evaluator = ImageNetEvaluator(self._config['dataset_dir'], image_size=image_net_config.dataset['image_size'],\n",
    "                                      batch_size=image_net_config.evaluation['batch_size'],\n",
    "                                      num_workers=image_net_config.evaluation['num_workers'])\n",
    "\n",
    "        return evaluator.evaluate(model, iterations, use_cuda)\n",
    "    \n",
    "    def finetune(self, model: torch.nn.Module):\n",
    "        \"\"\"\n",
    "        Finetunes the model.  The implemtation provided here is just an example,\n",
    "        provide your own implementation if needed.\n",
    "\n",
    "        :param model: The model to finetune.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "\n",
    "        # Your code goes here instead of the example from below\n",
    "\n",
    "        trainer = ImageNetTrainer(self._config['dataset_dir'], image_size=image_net_config.dataset['image_size'],\n",
    "                                  batch_size=image_net_config.train['batch_size'],\n",
    "                                  num_workers=image_net_config.train['num_workers'])\n",
    "\n",
    "        trainer.train(model, max_epochs=self._config['epochs'], learning_rate=self._config['learning_rate'],\n",
    "                      learning_rate_schedule=self._config['learning_rate_schedule'], use_cuda=self._config['use_cuda'])\n",
    "\n",
    "        torch.save(model, os.path.join(self._config['output_dir'], 'finetuned_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the pipeline and load the model\n",
    "We next initialize the pipeline and the model. Before compressing the model, it is customary to log the original accuracy of the model on the dataset provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipeline = ImageNetDataPipeline(config)\n",
    "\n",
    "model = resnet18(pretrained=True)\n",
    "if config['use_cuda']:\n",
    "    if torch.cuda.is_available():\n",
    "        model.to(torch.device('cuda'))\n",
    "    else:\n",
    "        raise Exception(\"use_cuda is True but cuda is unavailable\")\n",
    "\n",
    "accuracy = data_pipeline.evaluate(model, use_cuda=config['use_cuda'])\n",
    "print(\"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compression\n",
    "\n",
    "The next cells are for the actual compression. First, parameters related to the compression are specified in the following cell:\n",
    "\n",
    "1. **target_comp_ratio**: The desired compession ratio using Spatial SVD. This value denotes the desired compression % of the original model. To compress the model to 20% of its original size, use 0.2. This would compress the model by 80%. The default value that is given is 50%\n",
    "\n",
    "2. **num_comp_ratio_candidates**: The number of compression ratios used by the API at each layer. Note that the model will test multiple different compression ratios per layer to try to compress less-important layers more, in such a way such that the overall compression ratio is equal to target_comp_ratio. The specified value is 10, which means that for each layer, the API will try the values 0.1, 0.2, ... 1.0 as ratios.\n",
    "\n",
    "3. **cost_metric**: Determines in what way the model is evaluated - can either be compute (mac), or space (memory).\n",
    "\n",
    "4. **num_eval_iterations**: The number of batches of data used to evaluate a model while the model is compressing. It is set to 10 to speed up the compression, rather than using the whole dataset. More details are later in the notebook/elsewhere in the AIMET API documentation\n",
    "\n",
    "5. **modules_to_ignore**: The layers that should be ignored during compression. The first layer is ignored to preserve the way the input interacts with the model; if there are other layers that should be ignored, add them to the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_comp_ratio = Decimal(0.5)\n",
    "\n",
    "# num_comp_ratio_candidates is typically set to 10\n",
    "num_comp_ratio_candidates = 2 # Test\n",
    "\n",
    "cost_metric = CostMetric.mac\n",
    "\n",
    "# num_eval_iterations is typically set to 10\n",
    "num_eval_iterations = 1 # Test\n",
    "\n",
    "modules_to_ignore = [model.conv1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next set of cells create the actual parameters needed for Spatial SVD. There are two methods which can be used to choose parameters - Auto and Manual. For Auto, the only option is a greedy selection scheme, where the optimal compression ratio is selected for each layer among a set list of candidates to reach the target ratio (which was specified in the previous cell). For Manual, the compression ratios for each layer need to specified; as a general rule of thumb, if one is to use Manual, is to start with the ratios found by Auto Mode and use it as a starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Greedy selection parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_params = GreedySelectionParameters(target_comp_ratio=target_comp_ratio,\n",
    "                                          num_comp_ratio_candidates=num_comp_ratio_candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create auto mode Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = SpatialSvdParameters.Mode.auto\n",
    "\n",
    "auto_params = SpatialSvdParameters.AutoModeParams(greedy_select_params=greedy_params,\n",
    "                                                  modules_to_ignore=modules_to_ignore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Spatial SVD parameters with Auto Mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = SpatialSvdParameters(mode, auto_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Spatial SVD scheme and set the image shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme = CompressionScheme.spatial_svd\n",
    "\n",
    "image_shape = (1, image_net_config.dataset['image_channels'],\n",
    "               image_net_config.dataset['image_width'], image_net_config.dataset['image_height'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the model is compressed using AIMET's ModelCompressor paired with the parameters specified above. This returns both the new model, which is saved, as well as relevant statistics. Finally, the compressed model is evaluated on the dataset. Note here that the ModelCompressor evaluates the model while compressing using the same evaluate function that is in our data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_model, comp_stats = ModelCompressor.compress_model(model=model,\n",
    "                                                              eval_callback=data_pipeline.evaluate,\n",
    "                                                              eval_iterations=num_eval_iterations,\n",
    "                                                              input_shape=image_shape,\n",
    "                                                              compress_scheme=scheme,\n",
    "                                                              cost_metric=cost_metric,\n",
    "                                                              parameters=params)\n",
    "\n",
    "torch.save(compressed_model, os.path.join(config['output_dir'], 'compressed_model.pth'))\n",
    "\n",
    "print(\"comp_stats:\", comp_stats)\n",
    "\n",
    "comp_accuracy = data_pipeline.evaluate(compressed_model, use_cuda=config['use_cuda'])\n",
    "print(\"comp_accuracy:\", comp_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Finetuning\n",
    "\n",
    "After the model is compressed, the model is finetuned and evaluated again to get the finetuned accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipeline.finetune(compressed_model)\n",
    "\n",
    "finetuned_accuracy = data_pipeline.evaluate(compressed_model, use_cuda=config['use_cuda'])\n",
    "print(\"finetuned_accuracy:\", finetuned_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This example illustrates AIMET model compression and finetuning capability using Spatial SVD technique. To use AIMET for your specific needs, please replace the Resnet18 model used here with your model of interest and also replace the ImageNetDataPipeline class with your own data pipeline. This will provide with a quick starting point to understand the Spatial SVD capability in AIMET better. As indicated above, some parameters have been chosen in a way to run the example faster. Please experiment with them to get better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
