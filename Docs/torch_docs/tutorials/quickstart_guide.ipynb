{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cd6a28e-694c-4e67-9519-58690081186c",
   "metadata": {},
   "source": [
    "# Quickstart Guide\n",
    "\n",
    "In this tutorial, we will go through the end-to-end process of using AIMET and PyTorch to create, calibrate, and export a simple quantized model. Note that this is intended to show the most basic workflow in AIMET. It is *not* meant to demonstrate the most state-of-the-art techniques available in AIMET.\n",
    "\n",
    "\n",
    "## Overall flow\n",
    "\n",
    "1. Define the basic floating-point PyTorch model, training, and eval loops\n",
    "2. Prepare the trained model for quantization\n",
    "3. Create quantization simulation (quantsim) model in AIMET to simulate the effects of quantization\n",
    "4. Calibrate the quantsim model on training data and evaluate the quantized accuracy\n",
    "5. Fine-tune the quantized model to improve the quantized accuracy\n",
    "6. Export the quantized model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8850f4ee-ff9a-44c1-92e6-89a5465fe1a3",
   "metadata": {},
   "source": [
    "## PyTorch prerequisites\n",
    "\n",
    "To see clearly what happens inside AIMET, let's first start with some simple PyTorch code for defining, training, and evaluating a model. The code below is adapted from PyTorch's [basic optimization tutorial](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html). Note that AIMET does not have any special requirement on what these training/eval loops look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b878640a-a56c-461a-8931-296b9f87c688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1) Start with some data loaders to train, evaluate, and calibrate the model\n",
    "\n",
    "cifar10_train_data = torchvision.datasets.FashionMNIST('/tmp/cifar10', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "cifar10_test_data = torchvision.datasets.FashionMNIST('/tmp/cifar10', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar10_train_data, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(cifar10_train_data, batch_size=128, shuffle=True)\n",
    "\n",
    "# 2) Define a simple model to train on this dataset\n",
    "\n",
    "class Network(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=128, kernel_size=3, padding=1, stride=2)\n",
    "        self.bn_1 = torch.nn.BatchNorm2d(128)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1, stride=2)\n",
    "        self.bn_2 = torch.nn.BatchNorm2d(256)\n",
    "        self.linear = torch.nn.Linear(in_features=7*7*256, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn_1(x))\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn_2(x))\n",
    "        x = self.linear(x.view(x.shape[0], -1))\n",
    "        return F.softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "# 3) Define an evaluation loop for the model\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    for x, y in data_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        output = model(x)\n",
    "        correct += (torch.argmax(output, dim=1) == y).sum()\n",
    "        total += x.shape[0]\n",
    "\n",
    "    accuracy = correct / total * 100.\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc24cb27-14ba-4e48-a788-48ec8559dbf4",
   "metadata": {},
   "source": [
    "Now, let's instantiate a network and train for a few epochs on our dataset to establish a baseline floating-point model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b97ccbe3-b610-4c0d-b84d-6be5c3a2407d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Floating point accuracy: 91.08499908447266\n"
     ]
    }
   ],
   "source": [
    "# Create a model\n",
    "model = Network()\n",
    "\n",
    "# Send the model to the desired device (optional)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)    \n",
    "\n",
    "# Define some loss function and optimizer\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Train for 3 epochs\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        output = model(x)\n",
    "        loss = loss_fn(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "# Evaluate the floating-point model\n",
    "model.eval()\n",
    "fp_accuracy = evaluate(model, test_loader)\n",
    "print(f\"Floating point accuracy: {fp_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce057e4b-9bb2-49d1-98f6-94eb580da3cd",
   "metadata": {},
   "source": [
    "## Prepare the floating point model for quantization\n",
    "\n",
    "Before we can (accurately) simulate quantization, there are a couple important steps to take care of:\n",
    "\n",
    "\n",
    "#### 1) Model preparation\n",
    "\n",
    "AIMET's quantization simulation tool (`QuantizationSimModel`) expects the floating point model to conform to some specific guidelines. For example, `QuantizationSimModel` is only able to quantize math operations performed by `torch.nn.Module` objects, whereas `torch.nn.functional` calls will be (incorrectly) ignored. \n",
    "\n",
    "If we look back at our previous model definition, we see it calls `F.relu` and `F.softmax` in the forward function. Does this mean we need to completely redefine our model to use AIMET? Thankfully, no. AIMET provides the `model_preparer` API to transform our incompatible model into a new fully-compatible model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39f37a52-2acc-4064-a0d4-9f04087d314b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-01 19:13:45,446 - root - INFO - AIMET\n",
      "2024-05-01 19:13:45,504 - ModelPreparer - INFO - Functional         : Adding new module for node: {module_relu} \n",
      "2024-05-01 19:13:45,506 - ModelPreparer - INFO - Functional         : Adding new module for node: {module_relu_1} \n",
      "2024-05-01 19:13:45,506 - ModelPreparer - INFO - Functional         : Adding new module for node: {module_softmax} \n",
      "GraphModule(\n",
      "  (conv1): Conv2d(1, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (bn_1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (bn_2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear): Linear(in_features=12544, out_features=10, bias=True)\n",
      "  (module_relu): ReLU()\n",
      "  (module_relu_1): ReLU()\n",
      "  (module_softmax): Softmax(dim=-1)\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    conv1 = self.conv1(x);  x = None\n",
      "    bn_1 = self.bn_1(conv1);  conv1 = None\n",
      "    module_relu = self.module_relu(bn_1);  bn_1 = None\n",
      "    conv2 = self.conv2(module_relu);  module_relu = None\n",
      "    bn_2 = self.bn_2(conv2);  conv2 = None\n",
      "    module_relu_1 = self.module_relu_1(bn_2);  bn_2 = None\n",
      "    getattr_1 = module_relu_1.shape\n",
      "    getitem = getattr_1[0];  getattr_1 = None\n",
      "    view = module_relu_1.view(getitem, -1);  module_relu_1 = getitem = None\n",
      "    linear = self.linear(view);  view = None\n",
      "    module_softmax = self.module_softmax(linear);  linear = None\n",
      "    return module_softmax\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n"
     ]
    }
   ],
   "source": [
    "from aimet_torch import model_preparer\n",
    "\n",
    "prepared_model = model_preparer.prepare_model(model)\n",
    "print(prepared_model)\n",
    "\n",
    "# Note: This transformation should not change the model's forward function at all\n",
    "fp_accuracy_prepared = evaluate(prepared_model, test_loader)\n",
    "assert fp_accuracy_prepared == fp_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36627d75-4ea6-4b1e-9029-d5637cbc54ad",
   "metadata": {},
   "source": [
    "Note how the prepared model now contains distinct modules for the `relu` and `softmax` operations.\n",
    "\n",
    "#### 2) BatchNorm fold\n",
    "\n",
    "When models are executed in a quantized runtime, batchnorm layers are typically folded into the weight and bias of an adjacent convolution layer whenever possible in order to remove unnecessary computations. To accurately simulate inference in these runtimes, it is generally a good idea to perform this batchnorm folding on the floating point model before applying quantization. AIMET provides the `batch_norm_fold` tool to do this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eab7d735-9e7f-449c-bb37-6dc74a82e5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-01 19:13:54,376 - Utils - INFO - ...... subset to store [Conv_0, BatchNormalization_1]\n",
      "2024-05-01 19:13:54,377 - Utils - INFO - ...... subset to store [Conv_3, BatchNormalization_4]\n",
      "2024-05-01 19:13:54,450 - BatchNormFolding - INFO - 0 BatchNorms' weights got converted\n",
      "GraphModule(\n",
      "  (conv1): Conv2d(1, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (bn_1): Identity()\n",
      "  (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (bn_2): Identity()\n",
      "  (linear): Linear(in_features=12544, out_features=10, bias=True)\n",
      "  (module_relu): ReLU()\n",
      "  (module_relu_1): ReLU()\n",
      "  (module_softmax): Softmax(dim=-1)\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    conv1 = self.conv1(x);  x = None\n",
      "    bn_1 = self.bn_1(conv1);  conv1 = None\n",
      "    module_relu = self.module_relu(bn_1);  bn_1 = None\n",
      "    conv2 = self.conv2(module_relu);  module_relu = None\n",
      "    bn_2 = self.bn_2(conv2);  conv2 = None\n",
      "    module_relu_1 = self.module_relu_1(bn_2);  bn_2 = None\n",
      "    getattr_1 = module_relu_1.shape\n",
      "    getitem = getattr_1[0];  getattr_1 = None\n",
      "    view = module_relu_1.view(getitem, -1);  module_relu_1 = getitem = None\n",
      "    linear = self.linear(view);  view = None\n",
      "    module_softmax = self.module_softmax(linear);  linear = None\n",
      "    return module_softmax\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n"
     ]
    }
   ],
   "source": [
    "from aimet_torch import batch_norm_fold\n",
    "\n",
    "sample_input, _ = next(iter(train_loader))\n",
    "batch_norm_fold.fold_all_batch_norms(prepared_model, input_shapes=sample_input.shape)\n",
    "\n",
    "print(prepared_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b8305d-bb6a-4613-940c-786424c9fef5",
   "metadata": {},
   "source": [
    "Note that the model now has `Identity` (passthrough) layers where it previously had `BatchNorm2d` layers. Like the `model_preparer` step, this operation should not impact the model's accuracy.\n",
    "\n",
    "\n",
    "## Quantize the model\n",
    "\n",
    "Now, we are ready to use AIMET's `QuantizationSimModel` to simulate quantizing the floating point model. This involves two steps:\n",
    "\n",
    "1) Add quantizers to perform \"fake quantization\" during the model's forward pass\n",
    "2) Calibrate the quantizer encodings (e.g., min/max ranges) on some sample inputs\n",
    "\n",
    "Calibration is necessary to determine the range of values each activation quantizer is likely to encounter in the model's forward pass, and should therefore be able to represent. Theoretically, we could pass the entire training dataset through the model for calibration, but in practice we usually only need about 500-1000 representative samples to accurately estimate the ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69e60d51-6db3-4a1c-93ee-097915205ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-01 19:13:54,518 - Quant - INFO - Unsupported op type Squeeze\n",
      "2024-05-01 19:13:54,519 - Quant - INFO - Unsupported op type Mean\n",
      "2024-05-01 19:13:54,520 - Quant - INFO -  Set op level config for op = {Gemm}\n",
      "2024-05-01 19:13:54,521 - Utils - INFO - ...... subset to store [Conv_0, Relu_2]\n",
      "2024-05-01 19:13:54,522 - Utils - INFO - ...... subset to store [Conv_3, Relu_5]\n",
      "2024-05-01 19:13:54,522 - Quant - INFO - Selecting DefaultOpInstanceConfigGenerator to compute the specialized config. hw_version:default\n",
      "GraphModule(\n",
      "  (conv1): QuantizedConv2d(\n",
      "    1, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
      "    (param_quantizers): ModuleDict(\n",
      "      (weight): QuantizeDequantize(shape=[128, 1, 1, 1], bitwidth=4, symmetric=True)\n",
      "      (bias): None\n",
      "    )\n",
      "    (input_quantizers): ModuleList(\n",
      "      (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
      "    )\n",
      "    (output_quantizers): ModuleList(\n",
      "      (0): None\n",
      "    )\n",
      "  )\n",
      "  (bn_1): Identity()\n",
      "  (module_relu): FakeQuantizedReLU(\n",
      "    (param_quantizers): ModuleDict()\n",
      "    (input_quantizers): ModuleList(\n",
      "      (0): None\n",
      "    )\n",
      "    (output_quantizers): ModuleList(\n",
      "      (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
      "    )\n",
      "  )\n",
      "  (conv2): QuantizedConv2d(\n",
      "    128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
      "    (param_quantizers): ModuleDict(\n",
      "      (weight): QuantizeDequantize(shape=[256, 1, 1, 1], bitwidth=4, symmetric=True)\n",
      "      (bias): None\n",
      "    )\n",
      "    (input_quantizers): ModuleList(\n",
      "      (0): None\n",
      "    )\n",
      "    (output_quantizers): ModuleList(\n",
      "      (0): None\n",
      "    )\n",
      "  )\n",
      "  (bn_2): Identity()\n",
      "  (module_relu_1): FakeQuantizedReLU(\n",
      "    (param_quantizers): ModuleDict()\n",
      "    (input_quantizers): ModuleList(\n",
      "      (0): None\n",
      "    )\n",
      "    (output_quantizers): ModuleList(\n",
      "      (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
      "    )\n",
      "  )\n",
      "  (linear): QuantizedLinear(\n",
      "    in_features=12544, out_features=10, bias=True\n",
      "    (param_quantizers): ModuleDict(\n",
      "      (weight): QuantizeDequantize(shape=[1], bitwidth=4, symmetric=True)\n",
      "      (bias): None\n",
      "    )\n",
      "    (input_quantizers): ModuleList(\n",
      "      (0): None\n",
      "    )\n",
      "    (output_quantizers): ModuleList(\n",
      "      (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
      "    )\n",
      "  )\n",
      "  (module_softmax): QuantizedSoftmax(\n",
      "    dim=-1\n",
      "    (param_quantizers): ModuleDict()\n",
      "    (input_quantizers): ModuleList(\n",
      "      (0): None\n",
      "    )\n",
      "    (output_quantizers): ModuleList(\n",
      "      (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    conv1 = self.conv1(x);  x = None\n",
      "    bn_1 = self.bn_1(conv1);  conv1 = None\n",
      "    module_relu = self.module_relu(bn_1);  bn_1 = None\n",
      "    conv2 = self.conv2(module_relu);  module_relu = None\n",
      "    bn_2 = self.bn_2(conv2);  conv2 = None\n",
      "    module_relu_1 = self.module_relu_1(bn_2);  bn_2 = None\n",
      "    getattr_1 = module_relu_1.shape\n",
      "    getitem = getattr_1[0];  getattr_1 = None\n",
      "    view = module_relu_1.view(getitem, -1);  module_relu_1 = getitem = None\n",
      "    linear = self.linear(view);  view = None\n",
      "    module_softmax = self.module_softmax(linear);  linear = None\n",
      "    return module_softmax\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "Floating point model accuracy: 91.08499908447266 %\n",
      "Quantized model accuracy: 90.59333038330078 %\n"
     ]
    }
   ],
   "source": [
    "import aimet_torch.v2 as aimet\n",
    "from aimet_torch.v2 import quantsim\n",
    "\n",
    "# QuantizationSimModel will convert each nn.Module in prepared_model into a quantized equivalent module and configure the module's quantizers\n",
    "# In this case, we will quantize all parameters to 4 bits and all activations to 8 bits.\n",
    "sim = quantsim.QuantizationSimModel(prepared_model, \n",
    "                                    dummy_input=sample_input.to(device),\n",
    "                                    default_output_bw=8,                                # Simulate 8-bit activations\n",
    "                                    default_param_bw=4,                                 # Simulate 4-bit weights\n",
    "                                    config_file=quantsim.DEFAULT_PER_CHANNEL_CONFIG)    # User per-channel quantization for weights\n",
    "\n",
    "# Inside the compute_encodings context, quantizers will observe the statistics of the activations passing through them. These statistics will be used\n",
    "# to compute properly calibrated encodings upon exiting the context.\n",
    "with aimet.nn.compute_encodings(sim.model):\n",
    "    for idx, (x, _) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        sim.model(x)\n",
    "        if idx >= 10:\n",
    "            break\n",
    "\n",
    "# Compare the accuracy before and after quantization:\n",
    "quantized_accuracy = evaluate(sim.model, test_loader)\n",
    "\n",
    "print(sim.model)\n",
    "\n",
    "print(f\"Floating point model accuracy: {fp_accuracy} %\\n\"\n",
    "      f\"Quantized model accuracy: {quantized_accuracy} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8963c68c-ed16-414e-a99d-2b0593d9f2c7",
   "metadata": {},
   "source": [
    "Here, we can see that `sim.model` is nothing more than the `prepared_model` with every layer replaced with a quantized version of the layer. The quantization behavior of each module is determined by the configuration of its held quantizers. \n",
    "\n",
    "For example, we can see that `sim.model.conv2` has a 4-bit weight quantizer with shape `[256, 1, 1, 1]` (this is a per-channel parameter quantizer), and an 8-bit output quantizer with shape `[1]` (this is a per-tensor activation quantizer). We will discuss more about the exact meaning and motivation for these shapes in a later tutorial.\n",
    "\n",
    "## Fine-tune the model with quantization aware training\n",
    "\n",
    "If we're not satisfied with our accuracy after applying quantization, there are some steps we can take to further optimize the quantized accuracy. One such step is quantization aware training (QAT), during which the model is trained with the fake-quantization ops present. \n",
    "\n",
    "Let's repeat our floating-point training loop for one more epoch, but this time use the quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c201ca35-7ff2-48a7-a9a7-872bf1874e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original quantized model accuracy: 90.59333038330078 %\n",
      "Post-QAT model accuracy: 91.79999542236328 %\n"
     ]
    }
   ],
   "source": [
    "# Define some loss function and optimizer\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(sim.model.parameters(), lr=1e-4)\n",
    "\n",
    "# Train for one more epoch on the quantsim model\n",
    "for epoch in range(1):\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        output = sim.model(x)\n",
    "        loss = loss_fn(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "# Compare the accuracy before and after QAT:\n",
    "post_QAT_accuracy = evaluate(sim.model, test_loader)\n",
    "\n",
    "print(f\"Original quantized model accuracy: {quantized_accuracy} %\\n\"\n",
    "      f\"Post-QAT model accuracy: {post_QAT_accuracy} %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecd31da-0979-4509-b2ce-ce69d9c5a975",
   "metadata": {},
   "source": [
    "## Export the quantsim model\n",
    "\n",
    "Now that we are happy with our quantized model's accuracy, we are ready to export the model with its quantization parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3ea76c-b616-40b6-afa2-8a987c1cbeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_path = \".\"\n",
    "model_name = \"fashion_mnist_model\"\n",
    "sample_input, _ = next(iter(train_loader))\n",
    "\n",
    "sim.export(export_path, model_name, dummy_input=sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245dc746-1d72-4da6-9bb0-78f3ecd23fb0",
   "metadata": {},
   "source": [
    "This export method will save the model with quantization nodes removed, along with an encodings file containing quantization parameters for each activation and weight tensor in the model. These artifacts can then be sent to a quantized runtime such as Qualcomm® Neural Processing SDK."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
